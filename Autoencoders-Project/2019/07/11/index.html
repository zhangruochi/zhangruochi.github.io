<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhangruochi.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="In this project you will train your own autoencoder for human faces!">
<meta property="og:type" content="article">
<meta property="og:title" content="Autoencoders Project">
<meta property="og:url" content="https://zhangruochi.com/Autoencoders-Project/2019/07/11/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:description" content="In this project you will train your own autoencoder for human faces!">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zhangruochi.com/Autoencoders-Project/2019/07/11/output_9_2.png">
<meta property="og:image" content="https://zhangruochi.com/Autoencoders-Project/2019/07/11/autoencoder.png">
<meta property="og:image" content="https://zhangruochi.com/Autoencoders-Project/2019/07/11/pca.png">
<meta property="og:image" content="https://zhangruochi.com/Autoencoders-Project/2019/07/11/output_16_1.png">
<meta property="og:image" content="https://zhangruochi.com/Autoencoders-Project/2019/07/11/output_16_2.png">
<meta property="og:image" content="https://zhangruochi.com/Autoencoders-Project/2019/07/11/output_16_3.png">
<meta property="og:image" content="https://zhangruochi.com/Autoencoders-Project/2019/07/11/output_16_4.png">
<meta property="og:image" content="https://zhangruochi.com/Autoencoders-Project/2019/07/11/output_16_5.png">
<meta property="og:image" content="https://zhangruochi.com/Autoencoders-Project/2019/07/11/transpose_conv.jpg">
<meta property="og:image" content="https://zhangruochi.com/Autoencoders-Project/2019/07/11/output_26_1.png">
<meta property="og:image" content="https://zhangruochi.com/Autoencoders-Project/2019/07/11/output_26_2.png">
<meta property="og:image" content="https://zhangruochi.com/Autoencoders-Project/2019/07/11/output_26_3.png">
<meta property="og:image" content="https://zhangruochi.com/Autoencoders-Project/2019/07/11/output_26_4.png">
<meta property="og:image" content="https://zhangruochi.com/Autoencoders-Project/2019/07/11/output_26_5.png">
<meta property="og:image" content="https://zhangruochi.com/Autoencoders-Project/2019/07/11/denoising.jpg">
<meta property="og:image" content="https://zhangruochi.com/Autoencoders-Project/2019/07/11/output_34_0.png">
<meta property="og:image" content="https://zhangruochi.com/Autoencoders-Project/2019/07/11/similar_images.jpg">
<meta property="article:published_time" content="2019-07-10T19:55:32.000Z">
<meta property="article:modified_time" content="2020-01-31T05:22:49.000Z">
<meta property="article:author" content="Ruochi Zhang">
<meta property="article:tag" content="Project">
<meta property="article:tag" content="Computer Vision">
<meta property="article:tag" content="Autoencoder">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhangruochi.com/Autoencoders-Project/2019/07/11/output_9_2.png">

<link rel="canonical" href="https://zhangruochi.com/Autoencoders-Project/2019/07/11/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Autoencoders Project | RUOCHI.AI</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">RUOCHI.AI</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

      
    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Autoencoders-Project/2019/07/11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Autoencoders Project
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-07-11 03:55:32" itemprop="dateCreated datePublished" datetime="2019-07-11T03:55:32+08:00">2019-07-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-31 13:22:49" itemprop="dateModified" datetime="2020-01-31T13:22:49+08:00">2020-01-31</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Autoencoders-Project/2019/07/11/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Autoencoders-Project/2019/07/11/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
            <div class="post-description">In this project you will train your own autoencoder for human faces!</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="denoising-autoencoders-and-where-to-find-them">Denoising Autoencoders And Where To Find Them</h1>
<p>Today we're going to train deep autoencoders and apply them to faces and similar images search.</p>
<p>Our new test subjects are human faces from the <a target="_blank" rel="noopener" href="http://vis-www.cs.umass.edu/lfw/">lfw dataset</a>.</p>
<h1 id="import-stuff">Import stuff</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&quot;..&quot;</span>)</span><br><span class="line"><span class="keyword">import</span> grading</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> keras, keras.layers <span class="keyword">as</span> L, keras.backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> lfw_dataset <span class="keyword">import</span> load_lfw_dataset</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> download_utils</span><br><span class="line"><span class="keyword">import</span> keras_utils</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras_utils <span class="keyword">import</span> reset_tf_session</span><br></pre></td></tr></table></figure>
<pre><code>Using TensorFlow backend.</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !!! remember to clear session/graph if you rebuild your graph to avoid out-of-memory errors !!!</span></span><br></pre></td></tr></table></figure>
<h1 id="load-dataset">Load dataset</h1>
<p>Dataset was downloaded for you. Relevant links (just in case): - http://www.cs.columbia.edu/CAVE/databases/pubfig/download/lfw_attributes.txt - http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz - http://vis-www.cs.umass.edu/lfw/lfw.tgz</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># we downloaded them for you, just link them here</span></span><br><span class="line">download_utils.link_week_4_resources()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load images</span></span><br><span class="line">X, attr = load_lfw_dataset(use_raw=<span class="literal">True</span>, dimx=<span class="number">32</span>, dimy=<span class="number">32</span>)</span><br><span class="line">IMG_SHAPE = X.shape[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># center images</span></span><br><span class="line">X = X.astype(<span class="string">&#x27;float32&#x27;</span>) / <span class="number">255.0</span> - <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># split</span></span><br><span class="line">X_train, X_test = train_test_split(X, test_size=<span class="number">0.1</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(IntProgress(value=0, max=18983), HTML(value=&#39;&#39;)))</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_image</span>(<span class="params">x</span>):</span></span><br><span class="line">    plt.imshow(np.clip(x + <span class="number">0.5</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">&#x27;sample images&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">3</span>,i+<span class="number">1</span>)</span><br><span class="line">    show_image(X[i])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X shape:&quot;</span>, X.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;attr shape:&quot;</span>, attr.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># try to free memory</span></span><br><span class="line"><span class="keyword">del</span> X</span><br><span class="line"><span class="keyword">import</span> gc</span><br><span class="line">gc.collect()</span><br></pre></td></tr></table></figure>
<pre><code>X shape: (13143, 32, 32, 3)
attr shape: (13143, 73)





2997</code></pre>
<figure>
<img src="output_9_2.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<h1 id="autoencoder-architecture">Autoencoder architecture</h1>
<p>Let's design autoencoder as two sequential keras models: the encoder and decoder respectively.</p>
<p>We will then use symbolic API to apply and train these models.</p>
<p><img src="autoencoder.png" style="width:50%"></p>
<h1 id="first-step-pca">First step: PCA</h1>
<p>Principial Component Analysis is a popular dimensionality reduction method.</p>
<p>Under the hood, PCA attempts to decompose object-feature matrix <span class="math inline">\(X\)</span> into two smaller matrices: <span class="math inline">\(W\)</span> and <span class="math inline">\(\hat W\)</span> minimizing <em>mean squared error</em>:</p>
<p><span class="math display">\[\|(X W) \hat{W} - X\|^2_2 \to_{W, \hat{W}} \min\]</span> - <span class="math inline">\(X \in \mathbb{R}^{n \times m}\)</span> - object matrix (<strong>centered</strong>); - <span class="math inline">\(W \in \mathbb{R}^{m \times d}\)</span> - matrix of direct transformation; - <span class="math inline">\(\hat{W} \in \mathbb{R}^{d \times m}\)</span> - matrix of reverse transformation; - <span class="math inline">\(n\)</span> samples, <span class="math inline">\(m\)</span> original dimensions and <span class="math inline">\(d\)</span> target dimensions;</p>
<p>In geometric terms, we want to find d axes along which most of variance occurs. The "natural" axes, if you wish.</p>
<p><img src="pca.png" style="width:30%"></p>
<p>PCA can also be seen as a special case of an autoencoder.</p>
<ul>
<li><strong>Encoder</strong>: X -&gt; Dense(d units) -&gt; code</li>
<li><strong>Decoder</strong>: code -&gt; Dense(m units) -&gt; X</li>
</ul>
<p>Where Dense is a fully-connected layer with linear activaton: $f(X) = W X + b $</p>
<p>Note: the bias term in those layers is responsible for "centering" the matrix i.e. substracting mean.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_pca_autoencoder</span>(<span class="params">img_shape, code_size</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Here we define a simple linear autoencoder as described above.</span></span><br><span class="line"><span class="string">    We also flatten and un-flatten data to be compatible with image shapes</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    encoder = keras.models.Sequential()</span><br><span class="line">    encoder.add(L.InputLayer(img_shape))</span><br><span class="line">    encoder.add(L.Flatten())                  <span class="comment">#flatten image to vector</span></span><br><span class="line">    encoder.add(L.Dense(code_size))           <span class="comment">#actual encoder</span></span><br><span class="line"></span><br><span class="line">    decoder = keras.models.Sequential()</span><br><span class="line">    decoder.add(L.InputLayer((code_size,)))</span><br><span class="line">    decoder.add(L.Dense(np.prod(img_shape)))  <span class="comment">#actual decoder, height*width*3 units</span></span><br><span class="line">    decoder.add(L.Reshape(img_shape))         <span class="comment">#un-flatten</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> encoder,decoder</span><br></pre></td></tr></table></figure>
<p>Meld them together into one model:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">s = reset_tf_session()</span><br><span class="line"></span><br><span class="line">encoder, decoder = build_pca_autoencoder(IMG_SHAPE, code_size=<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line">inp = L.Input(IMG_SHAPE)</span><br><span class="line">code = encoder(inp)</span><br><span class="line">reconstruction = decoder(code)</span><br><span class="line"></span><br><span class="line">autoencoder = keras.models.Model(inputs=inp, outputs=reconstruction)</span><br><span class="line">autoencoder.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adamax&#x27;</span>, loss=<span class="string">&#x27;mse&#x27;</span>)</span><br><span class="line"></span><br><span class="line">autoencoder.fit(x=X_train, y=X_train, epochs=<span class="number">15</span>,</span><br><span class="line">                validation_data=[X_test, X_test],</span><br><span class="line">                callbacks=[keras_utils.TqdmProgressCallback()],</span><br><span class="line">                verbose=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/15



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))




Epoch 2/15



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))




Epoch 3/15



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))




Epoch 4/15



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))




Epoch 5/15



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))




Epoch 6/15



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))




Epoch 7/15



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))




Epoch 8/15



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))




Epoch 9/15



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))




Epoch 10/15



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))




Epoch 11/15



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))




Epoch 12/15



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))




Epoch 13/15



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))




Epoch 14/15



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))




Epoch 15/15



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))








&lt;keras.callbacks.History at 0x7f769160d550&gt;</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize</span>(<span class="params">img,encoder,decoder</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Draws original, encoded and decoded images&quot;&quot;&quot;</span></span><br><span class="line">    code = encoder.predict(img[<span class="literal">None</span>])[<span class="number">0</span>]  <span class="comment"># img[None] is the same as img[np.newaxis, :]</span></span><br><span class="line">    reco = decoder.predict(code[<span class="literal">None</span>])[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">1</span>,<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;Original&quot;</span>)</span><br><span class="line">    show_image(img)</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;Code&quot;</span>)</span><br><span class="line">    plt.imshow(code.reshape([code.shape[-<span class="number">1</span>]//<span class="number">2</span>,-<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;Reconstructed&quot;</span>)</span><br><span class="line">    show_image(reco)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">score = autoencoder.evaluate(X_test,X_test,verbose=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;PCA MSE:&quot;</span>, score)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    img = X_test[i]</span><br><span class="line">    visualize(img,encoder,decoder)</span><br></pre></td></tr></table></figure>
<pre><code>PCA MSE: 0.00662136772442</code></pre>
<figure>
<img src="output_16_1.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<figure>
<img src="output_16_2.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<figure>
<img src="output_16_3.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<figure>
<img src="output_16_4.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<figure>
<img src="output_16_5.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<h1 id="going-deeper-convolutional-autoencoder">Going deeper: convolutional autoencoder</h1>
<p>PCA is neat but surely we can do better. This time we want you to build a deep convolutional autoencoder by... stacking more layers.</p>
<h2 id="encoder">Encoder</h2>
<p>The <strong>encoder</strong> part is pretty standard, we stack convolutional and pooling layers and finish with a dense layer to get the representation of desirable size (<code>code_size</code>).</p>
<p>We recommend to use <code>activation='elu'</code> for all convolutional and dense layers.</p>
<p>We recommend to repeat (conv, pool) 4 times with kernel size (3, 3), <code>padding='same'</code> and the following numbers of output channels: <code>32, 64, 128, 256</code>.</p>
<p>Remember to flatten (<code>L.Flatten()</code>) output before adding the last dense layer!</p>
<h2 id="decoder">Decoder</h2>
<p>For <strong>decoder</strong> we will use so-called "transpose convolution".</p>
<p>Traditional convolutional layer takes a patch of an image and produces a number (patch -&gt; number). In "transpose convolution" we want to take a number and produce a patch of an image (number -&gt; patch). We need this layer to "undo" convolutions in encoder. We had a glimpse of it during week 3 (watch <a target="_blank" rel="noopener" href="https://www.coursera.org/learn/intro-to-deep-learning/lecture/auRqf/a-glimpse-of-other-computer-vision-tasks">this video</a> starting at 5:41).</p>
<p>Here's how "transpose convolution" works: <img src="transpose_conv.jpg" style="width:60%"> In this example we use a stride of 2 to produce 4x4 output, this way we "undo" pooling as well. Another way to think about it: we "undo" convolution with stride 2 (which is similar to conv + pool).</p>
<p>You can add "transpose convolution" layer in Keras like this: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">L.Conv2DTranspose(filters=?, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">2</span>, activation=<span class="string">&#x27;elu&#x27;</span>, padding=<span class="string">&#x27;same&#x27;</span>)</span><br></pre></td></tr></table></figure></p>
<p>Our decoder starts with a dense layer to "undo" the last layer of encoder. Remember to reshape its output to "undo" <code>L.Flatten()</code> in encoder.</p>
<p>Now we're ready to undo (conv, pool) pairs. For this we need to stack 4 <code>L.Conv2DTranspose</code> layers with the following numbers of output channels: <code>128, 64, 32, 3</code>. Each of these layers will learn to "undo" (conv, pool) pair in encoder. For the last <code>L.Conv2DTranspose</code> layer use <code>activation=None</code> because that is our final image.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let&#x27;s play around with transpose convolution on examples first</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_conv2d_transpose</span>(<span class="params">img_size, filter_size</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Transpose convolution test for img_size=&#123;&#125;, filter_size=&#123;&#125;:&quot;</span>.<span class="built_in">format</span>(img_size, filter_size))</span><br><span class="line">    </span><br><span class="line">    x = (np.arange(img_size ** <span class="number">2</span>, dtype=np.float32) + <span class="number">1</span>).reshape((<span class="number">1</span>, img_size, img_size, <span class="number">1</span>))</span><br><span class="line">    f = (np.ones(filter_size ** <span class="number">2</span>, dtype=np.float32)).reshape((filter_size, filter_size, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    s = reset_tf_session()</span><br><span class="line">    </span><br><span class="line">    conv = tf.nn.conv2d_transpose(x, f, </span><br><span class="line">                                  output_shape=(<span class="number">1</span>, img_size * <span class="number">2</span>, img_size * <span class="number">2</span>, <span class="number">1</span>), </span><br><span class="line">                                  strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], </span><br><span class="line">                                  padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    result = s.run(conv)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;input:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(x[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;filter:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(f[:, :, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;output:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(result[<span class="number">0</span>, :, :, <span class="number">0</span>])</span><br><span class="line">    s.close()</span><br><span class="line">        </span><br><span class="line">test_conv2d_transpose(img_size=<span class="number">2</span>, filter_size=<span class="number">2</span>)</span><br><span class="line">test_conv2d_transpose(img_size=<span class="number">2</span>, filter_size=<span class="number">3</span>)</span><br><span class="line">test_conv2d_transpose(img_size=<span class="number">4</span>, filter_size=<span class="number">2</span>)</span><br><span class="line">test_conv2d_transpose(img_size=<span class="number">4</span>, filter_size=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Transpose convolution test for img_size=2, filter_size=2:
input:
[[ 1.  2.]
 [ 3.  4.]]
filter:
[[ 1.  1.]
 [ 1.  1.]]
output:
[[ 1.  1.  2.  2.]
 [ 1.  1.  2.  2.]
 [ 3.  3.  4.  4.]
 [ 3.  3.  4.  4.]]
Transpose convolution test for img_size=2, filter_size=3:
input:
[[ 1.  2.]
 [ 3.  4.]]
filter:
[[ 1.  1.  1.]
 [ 1.  1.  1.]
 [ 1.  1.  1.]]
output:
[[  1.   1.   3.   2.]
 [  1.   1.   3.   2.]
 [  4.   4.  10.   6.]
 [  3.   3.   7.   4.]]
Transpose convolution test for img_size=4, filter_size=2:
input:
[[  1.   2.   3.   4.]
 [  5.   6.   7.   8.]
 [  9.  10.  11.  12.]
 [ 13.  14.  15.  16.]]
filter:
[[ 1.  1.]
 [ 1.  1.]]
output:
[[  1.   1.   2.   2.   3.   3.   4.   4.]
 [  1.   1.   2.   2.   3.   3.   4.   4.]
 [  5.   5.   6.   6.   7.   7.   8.   8.]
 [  5.   5.   6.   6.   7.   7.   8.   8.]
 [  9.   9.  10.  10.  11.  11.  12.  12.]
 [  9.   9.  10.  10.  11.  11.  12.  12.]
 [ 13.  13.  14.  14.  15.  15.  16.  16.]
 [ 13.  13.  14.  14.  15.  15.  16.  16.]]
Transpose convolution test for img_size=4, filter_size=3:
input:
[[  1.   2.   3.   4.]
 [  5.   6.   7.   8.]
 [  9.  10.  11.  12.]
 [ 13.  14.  15.  16.]]
filter:
[[ 1.  1.  1.]
 [ 1.  1.  1.]
 [ 1.  1.  1.]]
output:
[[  1.   1.   3.   2.   5.   3.   7.   4.]
 [  1.   1.   3.   2.   5.   3.   7.   4.]
 [  6.   6.  14.   8.  18.  10.  22.  12.]
 [  5.   5.  11.   6.  13.   7.  15.   8.]
 [ 14.  14.  30.  16.  34.  18.  38.  20.]
 [  9.   9.  19.  10.  21.  11.  23.  12.]
 [ 22.  22.  46.  24.  50.  26.  54.  28.]
 [ 13.  13.  27.  14.  29.  15.  31.  16.]]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_deep_autoencoder</span>(<span class="params">img_shape, code_size</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;PCA&#x27;s deeper brother. See instructions above. Use `code_size` in layer definitions.&quot;&quot;&quot;</span></span><br><span class="line">    H,W,C = img_shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># encoder</span></span><br><span class="line">    encoder = keras.models.Sequential()</span><br><span class="line">    encoder.add(L.InputLayer(img_shape))</span><br><span class="line">    encoder.add(L.Conv2D(filters=<span class="number">32</span>, kernel_size=(<span class="number">3</span>,<span class="number">3</span>),padding=<span class="string">&quot;same&quot;</span>,activation=<span class="string">&quot;elu&quot;</span>))</span><br><span class="line">    encoder.add(L.MaxPooling2D())</span><br><span class="line">    encoder.add(L.Conv2D(filters=<span class="number">64</span>, kernel_size=(<span class="number">3</span>,<span class="number">3</span>),padding=<span class="string">&quot;same&quot;</span>,activation=<span class="string">&quot;elu&quot;</span>))</span><br><span class="line">    encoder.add(L.MaxPooling2D())</span><br><span class="line">    encoder.add(L.Conv2D(filters=<span class="number">128</span>, kernel_size=(<span class="number">3</span>,<span class="number">3</span>),padding=<span class="string">&quot;same&quot;</span>,activation=<span class="string">&quot;elu&quot;</span>))</span><br><span class="line">    encoder.add(L.MaxPooling2D())</span><br><span class="line">    encoder.add(L.Conv2D(filters=<span class="number">256</span>, kernel_size=(<span class="number">3</span>,<span class="number">3</span>),padding=<span class="string">&quot;same&quot;</span>,activation=<span class="string">&quot;elu&quot;</span>))</span><br><span class="line">    encoder.add(L.MaxPooling2D())</span><br><span class="line">    encoder.add(L.Flatten())</span><br><span class="line">    encoder.add(L.Dense(code_size))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### YOUR CODE HERE: define encoder as per instructions above ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># decoder</span></span><br><span class="line">    decoder = keras.models.Sequential()</span><br><span class="line">    decoder.add(L.InputLayer((code_size,)))</span><br><span class="line">    decoder.add(L.Dense(<span class="number">2</span>*<span class="number">2</span>*<span class="number">256</span>))</span><br><span class="line">    decoder.add(L.Reshape((<span class="number">2</span>, <span class="number">2</span>, <span class="number">256</span>)))</span><br><span class="line">    decoder.add(L.Conv2DTranspose(filters=<span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">2</span>, activation=<span class="string">&#x27;elu&#x27;</span>, padding=<span class="string">&#x27;same&#x27;</span>))</span><br><span class="line">    decoder.add(L.Conv2DTranspose(filters=<span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">2</span>, activation=<span class="string">&#x27;elu&#x27;</span>, padding=<span class="string">&#x27;same&#x27;</span>))</span><br><span class="line">    decoder.add(L.Conv2DTranspose(filters=<span class="number">32</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">2</span>, activation=<span class="string">&#x27;elu&#x27;</span>, padding=<span class="string">&#x27;same&#x27;</span>))</span><br><span class="line">    decoder.add(L.Conv2DTranspose(filters=<span class="number">3</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">2</span>, activation=<span class="literal">None</span>, padding=<span class="string">&#x27;same&#x27;</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### YOUR CODE HERE: define decoder as per instructions above ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> encoder, decoder</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Check autoencoder shapes along different code_sizes</span></span><br><span class="line">get_dim = <span class="keyword">lambda</span> layer: np.prod(layer.output_shape[<span class="number">1</span>:])</span><br><span class="line"><span class="keyword">for</span> code_size <span class="keyword">in</span> [<span class="number">1</span>,<span class="number">8</span>,<span class="number">32</span>,<span class="number">128</span>,<span class="number">512</span>]:</span><br><span class="line">    s = reset_tf_session()</span><br><span class="line">    encoder, decoder = build_deep_autoencoder(IMG_SHAPE, code_size=code_size)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Testing code size %i&quot;</span> % code_size)</span><br><span class="line">    <span class="keyword">assert</span> encoder.output_shape[<span class="number">1</span>:]==(code_size,),<span class="string">&quot;encoder must output a code of required size&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> decoder.output_shape[<span class="number">1</span>:]==IMG_SHAPE,   <span class="string">&quot;decoder must output an image of valid shape&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(encoder.trainable_weights)&gt;=<span class="number">6</span>,     <span class="string">&quot;encoder must contain at least 3 layers&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(decoder.trainable_weights)&gt;=<span class="number">6</span>,     <span class="string">&quot;decoder must contain at least 3 layers&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> encoder.layers + decoder.layers:</span><br><span class="line">        <span class="keyword">assert</span> get_dim(layer) &gt;= code_size, <span class="string">&quot;Encoder layer %s is smaller than bottleneck (%i units)&quot;</span>%(layer.name,get_dim(layer))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;All tests passed!&quot;</span>)</span><br><span class="line">s = reset_tf_session()</span><br></pre></td></tr></table></figure>
<pre><code>Testing code size 1
Testing code size 8
Testing code size 32
Testing code size 128
Testing code size 512
All tests passed!</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Look at encoder and decoder shapes.</span></span><br><span class="line"><span class="comment"># Total number of trainable parameters of encoder and decoder should be close.</span></span><br><span class="line">s = reset_tf_session()</span><br><span class="line">encoder, decoder = build_deep_autoencoder(IMG_SHAPE, code_size=<span class="number">32</span>)</span><br><span class="line">encoder.summary()</span><br><span class="line">decoder.summary()</span><br></pre></td></tr></table></figure>
<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 32, 32, 3)         0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 8, 8, 128)         73856     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 4, 4, 256)         295168    
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 2, 2, 256)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 1024)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                32800     
=================================================================
Total params: 421,216
Trainable params: 421,216
Non-trainable params: 0
_________________________________________________________________
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         (None, 32)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 1024)              33792     
_________________________________________________________________
reshape_1 (Reshape)          (None, 2, 2, 256)         0         
_________________________________________________________________
conv2d_transpose_1 (Conv2DTr (None, 4, 4, 128)         295040    
_________________________________________________________________
conv2d_transpose_2 (Conv2DTr (None, 8, 8, 64)          73792     
_________________________________________________________________
conv2d_transpose_3 (Conv2DTr (None, 16, 16, 32)        18464     
_________________________________________________________________
conv2d_transpose_4 (Conv2DTr (None, 32, 32, 3)         867       
=================================================================
Total params: 421,955
Trainable params: 421,955
Non-trainable params: 0
_________________________________________________________________</code></pre>
<p>Convolutional autoencoder training. This will take <strong>1 hour</strong>. You're aiming at ~0.0056 validation MSE and ~0.0054 training MSE.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">s = reset_tf_session()</span><br><span class="line"></span><br><span class="line">encoder, decoder = build_deep_autoencoder(IMG_SHAPE, code_size=<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line">inp = L.Input(IMG_SHAPE)</span><br><span class="line">code = encoder(inp)</span><br><span class="line">reconstruction = decoder(code)</span><br><span class="line"></span><br><span class="line">autoencoder = keras.models.Model(inputs=inp, outputs=reconstruction)</span><br><span class="line">autoencoder.<span class="built_in">compile</span>(optimizer=<span class="string">&quot;adamax&quot;</span>, loss=<span class="string">&#x27;mse&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># we will save model checkpoints here to continue training in case of kernel death</span></span><br><span class="line">model_filename = <span class="string">&#x27;autoencoder.&#123;0:03d&#125;.hdf5&#x27;</span></span><br><span class="line">last_finished_epoch = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### uncomment below to continue training from model checkpoint</span></span><br><span class="line"><span class="comment">#### fill `last_finished_epoch` with your latest finished epoch</span></span><br><span class="line"><span class="comment"># from keras.models import load_model</span></span><br><span class="line"><span class="comment"># s = reset_tf_session()</span></span><br><span class="line"><span class="comment"># last_finished_epoch = 4</span></span><br><span class="line"><span class="comment"># autoencoder = load_model(model_filename.format(last_finished_epoch))</span></span><br><span class="line"><span class="comment"># encoder = autoencoder.layers[1]</span></span><br><span class="line"><span class="comment"># decoder = autoencoder.layers[2]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">autoencoder.fit(x=X_train, y=X_train, epochs=<span class="number">25</span>,</span><br><span class="line">                validation_data=[X_test, X_test],</span><br><span class="line">                callbacks=[keras_utils.ModelSaveCallback(model_filename),</span><br><span class="line">                           keras_utils.TqdmProgressCallback()],</span><br><span class="line">                verbose=<span class="number">0</span>,</span><br><span class="line">                initial_epoch=last_finished_epoch <span class="keyword">or</span> <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/25



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))


Model saved in autoencoder.000.hdf5


Epoch 2/25



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))


Model saved in autoencoder.001.hdf5


Epoch 3/25



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))


Model saved in autoencoder.002.hdf5


Epoch 4/25



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))


Model saved in autoencoder.003.hdf5


Epoch 5/25



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))


Model saved in autoencoder.004.hdf5


Epoch 6/25



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))


Model saved in autoencoder.005.hdf5


Epoch 7/25



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))


Model saved in autoencoder.006.hdf5


Epoch 8/25



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))


Model saved in autoencoder.007.hdf5


Epoch 9/25



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))


Model saved in autoencoder.008.hdf5


Epoch 10/25



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))


Model saved in autoencoder.009.hdf5


Epoch 11/25



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))


Model saved in autoencoder.010.hdf5


Epoch 12/25



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))


Model saved in autoencoder.011.hdf5


Epoch 13/25



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))


Model saved in autoencoder.012.hdf5


Epoch 14/25



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))


Model saved in autoencoder.013.hdf5


Epoch 15/25



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))


Model saved in autoencoder.014.hdf5


Epoch 16/25



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))


Model saved in autoencoder.015.hdf5


Epoch 17/25



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))


Model saved in autoencoder.016.hdf5


Epoch 18/25



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))


Model saved in autoencoder.017.hdf5


Epoch 19/25



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))


Model saved in autoencoder.018.hdf5


Epoch 20/25



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))


Model saved in autoencoder.019.hdf5


Epoch 21/25



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))


Model saved in autoencoder.020.hdf5


Epoch 22/25



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))


Model saved in autoencoder.021.hdf5


Epoch 23/25



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))


Model saved in autoencoder.022.hdf5


Epoch 24/25



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))


Model saved in autoencoder.023.hdf5


Epoch 25/25



HBox(children=(IntProgress(value=0, max=11828), HTML(value=&#39;&#39;)))


Model saved in autoencoder.024.hdf5






&lt;keras.callbacks.History at 0x7f7691fe4710&gt;</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">reconstruction_mse = autoencoder.evaluate(X_test, X_test, verbose=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Convolutional autoencoder MSE:&quot;</span>, reconstruction_mse)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    img = X_test[i]</span><br><span class="line">    visualize(img,encoder,decoder)</span><br></pre></td></tr></table></figure>
<pre><code>Convolutional autoencoder MSE: 0.00544933940447</code></pre>
<figure>
<img src="output_26_1.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<figure>
<img src="output_26_2.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<figure>
<img src="output_26_3.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<figure>
<img src="output_26_4.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<figure>
<img src="output_26_5.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># save trained weights</span></span><br><span class="line">encoder.save_weights(<span class="string">&quot;encoder.h5&quot;</span>)</span><br><span class="line">decoder.save_weights(<span class="string">&quot;decoder.h5&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># restore trained weights</span></span><br><span class="line">s = reset_tf_session()</span><br><span class="line"></span><br><span class="line">encoder, decoder = build_deep_autoencoder(IMG_SHAPE, code_size=<span class="number">32</span>)</span><br><span class="line">encoder.load_weights(<span class="string">&quot;encoder.h5&quot;</span>)</span><br><span class="line">decoder.load_weights(<span class="string">&quot;decoder.h5&quot;</span>)</span><br><span class="line"></span><br><span class="line">inp = L.Input(IMG_SHAPE)</span><br><span class="line">code = encoder(inp)</span><br><span class="line">reconstruction = decoder(code)</span><br><span class="line"></span><br><span class="line">autoencoder = keras.models.Model(inputs=inp, outputs=reconstruction)</span><br><span class="line">autoencoder.<span class="built_in">compile</span>(optimizer=<span class="string">&quot;adamax&quot;</span>, loss=<span class="string">&#x27;mse&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(autoencoder.evaluate(X_test, X_test, verbose=<span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(reconstruction_mse)</span><br></pre></td></tr></table></figure>
<pre><code>0.00544933940447
0.00544933940447</code></pre>
<h1 id="submit-to-coursera">Submit to Coursera</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> submit <span class="keyword">import</span> submit_autoencoder</span><br><span class="line">submission = build_deep_autoencoder(IMG_SHAPE, code_size=<span class="number">71</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># token expires every 30 min</span></span><br><span class="line">COURSERA_TOKEN = <span class="string">&quot;LWCR4D3YtVhRBo86&quot;</span></span><br><span class="line">COURSERA_EMAIL = <span class="string">&quot;lvduzhen@gmail.com&quot;</span></span><br><span class="line"></span><br><span class="line">submit_autoencoder(submission, reconstruction_mse, COURSERA_EMAIL, COURSERA_TOKEN)</span><br></pre></td></tr></table></figure>
<pre><code>Submitted to Coursera platform. See results on assignment page!</code></pre>
<h1 id="optional-denoising-autoencoder">Optional: Denoising Autoencoder</h1>
<p>This part is <strong>optional</strong>, it shows you one useful application of autoencoders: denoising. You can run this code and make sure denoising works :)</p>
<p>Let's now turn our model into a denoising autoencoder: <img src="denoising.jpg" style="width:40%"></p>
<p>We'll keep the model architecture, but change the way it is trained. In particular, we'll corrupt its input data randomly with noise before each epoch.</p>
<p>There are many strategies to introduce noise: adding gaussian white noise, occluding with random black rectangles, etc. We will add gaussian white noise.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply_gaussian_noise</span>(<span class="params">X,sigma=<span class="number">0.1</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    adds noise from standard normal distribution with standard deviation sigma</span></span><br><span class="line"><span class="string">    :param X: image tensor of shape [batch,height,width,3]</span></span><br><span class="line"><span class="string">    Returns X + noise.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    noise =  np.random.normal(loc=<span class="number">0</span>,scale=sigma,size=X.shape)</span><br><span class="line">    <span class="keyword">return</span> X + noise</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># noise tests</span></span><br><span class="line">theoretical_std = (X_train[:<span class="number">100</span>].std()**<span class="number">2</span> + <span class="number">0.5</span>**<span class="number">2</span>)**<span class="number">.5</span></span><br><span class="line">our_std = apply_gaussian_noise(X_train[:<span class="number">100</span>],sigma=<span class="number">0.5</span>).std()</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">abs</span>(theoretical_std - our_std) &lt; <span class="number">0.01</span>, <span class="string">&quot;Standard deviation does not match it&#x27;s required value. Make sure you use sigma as std.&quot;</span></span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">abs</span>(apply_gaussian_noise(X_train[:<span class="number">100</span>],sigma=<span class="number">0.5</span>).mean() - X_train[:<span class="number">100</span>].mean()) &lt; <span class="number">0.01</span>, <span class="string">&quot;Mean has changed. Please add zero-mean noise&quot;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test different noise scales</span></span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line">show_image(X_train[<span class="number">0</span>])</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">4</span>,<span class="number">2</span>)</span><br><span class="line">show_image(apply_gaussian_noise(X_train[:<span class="number">1</span>],sigma=<span class="number">0.01</span>)[<span class="number">0</span>])</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line">show_image(apply_gaussian_noise(X_train[:<span class="number">1</span>],sigma=<span class="number">0.1</span>)[<span class="number">0</span>])</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line">show_image(apply_gaussian_noise(X_train[:<span class="number">1</span>],sigma=<span class="number">0.5</span>)[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<figure>
<img src="output_34_0.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<p>Training will take <strong>1 hour</strong>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">s = reset_tf_session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># we use bigger code size here for better quality</span></span><br><span class="line">encoder, decoder = build_deep_autoencoder(IMG_SHAPE, code_size=<span class="number">512</span>)</span><br><span class="line"><span class="keyword">assert</span> encoder.output_shape[<span class="number">1</span>:]==(<span class="number">512</span>,), <span class="string">&quot;encoder must output a code of required size&quot;</span></span><br><span class="line"></span><br><span class="line">inp = L.Input(IMG_SHAPE)</span><br><span class="line">code = encoder(inp)</span><br><span class="line">reconstruction = decoder(code)</span><br><span class="line"></span><br><span class="line">autoencoder = keras.models.Model(inp, reconstruction)</span><br><span class="line">autoencoder.<span class="built_in">compile</span>(<span class="string">&#x27;adamax&#x27;</span>, <span class="string">&#x27;mse&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">25</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch %i/25, Generating corrupted samples...&quot;</span>%(i+<span class="number">1</span>))</span><br><span class="line">    X_train_noise = apply_gaussian_noise(X_train)</span><br><span class="line">    X_test_noise = apply_gaussian_noise(X_test)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># we continue to train our model with new noise-augmented data</span></span><br><span class="line">    autoencoder.fit(x=X_train_noise, y=X_train, epochs=<span class="number">1</span>,</span><br><span class="line">                    validation_data=[X_test_noise, X_test],</span><br><span class="line">                    callbacks=[keras_utils.TqdmProgressCallback()],</span><br><span class="line">                    verbose=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X_test_noise = apply_gaussian_noise(X_test)</span><br><span class="line">denoising_mse = autoencoder.evaluate(X_test_noise, X_test, verbose=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Denoising MSE:&quot;</span>, denoising_mse)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    img = X_test_noise[i]</span><br><span class="line">    visualize(img,encoder,decoder)</span><br></pre></td></tr></table></figure>
<h1 id="optional-image-retrieval-with-autoencoders">Optional: Image retrieval with autoencoders</h1>
<p>So we've just trained a network that converts image into itself imperfectly. This task is not that useful in and of itself, but it has a number of awesome side-effects. Let's see them in action.</p>
<p>First thing we can do is image retrieval aka image search. We will give it an image and find similar images in latent space:</p>
<p><img src="similar_images.jpg" style="width:60%"></p>
<p>To speed up retrieval process, one should use Locality Sensitive Hashing on top of encoded vectors. This <a target="_blank" rel="noopener" href="https://erikbern.com/2015/07/04/benchmark-of-approximate-nearest-neighbor-libraries.html">technique</a> can narrow down the potential nearest neighbours of our image in latent space (encoder code). We will caclulate nearest neighbours in brute force way for simplicity.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># restore trained encoder weights</span></span><br><span class="line">s = reset_tf_session()</span><br><span class="line">encoder, decoder = build_deep_autoencoder(IMG_SHAPE, code_size=<span class="number">32</span>)</span><br><span class="line">encoder.load_weights(<span class="string">&quot;encoder.h5&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">images = X_train</span><br><span class="line">codes = encoder.predict(images)</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">len</span>(codes) == <span class="built_in">len</span>(images)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors.unsupervised <span class="keyword">import</span> NearestNeighbors</span><br><span class="line">nei_clf = NearestNeighbors(metric=<span class="string">&quot;euclidean&quot;</span>)</span><br><span class="line">nei_clf.fit(codes)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_similar</span>(<span class="params">image, n_neighbors=<span class="number">5</span></span>):</span></span><br><span class="line">    <span class="keyword">assert</span> image.ndim==<span class="number">3</span>,<span class="string">&quot;image must be [batch,height,width,3]&quot;</span></span><br><span class="line"></span><br><span class="line">    code = encoder.predict(image[<span class="literal">None</span>])</span><br><span class="line">    </span><br><span class="line">    (distances,),(idx,) = nei_clf.kneighbors(code,n_neighbors=n_neighbors)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> distances,images[idx]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_similar</span>(<span class="params">image</span>):</span></span><br><span class="line">    </span><br><span class="line">    distances,neighbors = get_similar(image,n_neighbors=<span class="number">3</span>)</span><br><span class="line">    </span><br><span class="line">    plt.figure(figsize=[<span class="number">8</span>,<span class="number">7</span>])</span><br><span class="line">    plt.subplot(<span class="number">1</span>,<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line">    show_image(image)</span><br><span class="line">    plt.title(<span class="string">&quot;Original image&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        plt.subplot(<span class="number">1</span>,<span class="number">4</span>,i+<span class="number">2</span>)</span><br><span class="line">        show_image(neighbors[i])</span><br><span class="line">        plt.title(<span class="string">&quot;Dist=%.3f&quot;</span>%distances[i])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>Cherry-picked examples:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># smiles</span></span><br><span class="line">show_similar(X_test[<span class="number">247</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ethnicity</span></span><br><span class="line">show_similar(X_test[<span class="number">56</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># glasses</span></span><br><span class="line">show_similar(X_test[<span class="number">63</span>])</span><br></pre></td></tr></table></figure>
<h1 id="optional-cheap-image-morphing">Optional: Cheap image morphing</h1>
<p>We can take linear combinations of image codes to produce new images with decoder.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># restore trained encoder weights</span></span><br><span class="line">s = reset_tf_session()</span><br><span class="line">encoder, decoder = build_deep_autoencoder(IMG_SHAPE, code_size=<span class="number">32</span>)</span><br><span class="line">encoder.load_weights(<span class="string">&quot;encoder.h5&quot;</span>)</span><br><span class="line">decoder.load_weights(<span class="string">&quot;decoder.h5&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    image1,image2 = X_test[np.random.randint(<span class="number">0</span>,<span class="built_in">len</span>(X_test),size=<span class="number">2</span>)]</span><br><span class="line"></span><br><span class="line">    code1, code2 = encoder.predict(np.stack([image1, image2]))</span><br><span class="line"></span><br><span class="line">    plt.figure(figsize=[<span class="number">10</span>,<span class="number">4</span>])</span><br><span class="line">    <span class="keyword">for</span> i,a <span class="keyword">in</span> <span class="built_in">enumerate</span>(np.linspace(<span class="number">0</span>,<span class="number">1</span>,num=<span class="number">7</span>)):</span><br><span class="line"></span><br><span class="line">        output_code = code1*(<span class="number">1</span>-a) + code2*(a)</span><br><span class="line">        output_image = decoder.predict(output_code[<span class="literal">None</span>])[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        plt.subplot(<span class="number">1</span>,<span class="number">7</span>,i+<span class="number">1</span>)</span><br><span class="line">        show_image(output_image)</span><br><span class="line">        plt.title(<span class="string">&quot;a=%.2f&quot;</span>%a)</span><br><span class="line">        </span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>That's it!</p>
<p>Of course there's a lot more you can do with autoencoders.</p>
<p>If you want to generate images from scratch, however, we recommend you our honor track on Generative Adversarial Networks or GANs.</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Project/" rel="tag"># Project</a>
              <a href="/tags/Computer-Vision/" rel="tag"># Computer Vision</a>
              <a href="/tags/Autoencoder/" rel="tag"># Autoencoder</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/Image-Captioning-Project/2019/07/11/" rel="prev" title="Image Captioning Project">
      <i class="fa fa-chevron-left"></i> Image Captioning Project
    </a></div>
      <div class="post-nav-item">
    <a href="/Generative-Adversarial-Network-Project/2019/07/11/" rel="next" title="Generative Adversarial Network Project">
      Generative Adversarial Network Project <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#denoising-autoencoders-and-where-to-find-them"><span class="nav-number">1.</span> <span class="nav-text">Denoising Autoencoders And Where To Find Them</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#import-stuff"><span class="nav-number">2.</span> <span class="nav-text">Import stuff</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#load-dataset"><span class="nav-number">3.</span> <span class="nav-text">Load dataset</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#autoencoder-architecture"><span class="nav-number">4.</span> <span class="nav-text">Autoencoder architecture</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#first-step-pca"><span class="nav-number">5.</span> <span class="nav-text">First step: PCA</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#going-deeper-convolutional-autoencoder"><span class="nav-number">6.</span> <span class="nav-text">Going deeper: convolutional autoencoder</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#encoder"><span class="nav-number">6.1.</span> <span class="nav-text">Encoder</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#decoder"><span class="nav-number">6.2.</span> <span class="nav-text">Decoder</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#submit-to-coursera"><span class="nav-number">7.</span> <span class="nav-text">Submit to Coursera</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#optional-denoising-autoencoder"><span class="nav-number">8.</span> <span class="nav-text">Optional: Denoising Autoencoder</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#optional-image-retrieval-with-autoencoders"><span class="nav-number">9.</span> <span class="nav-text">Optional: Image retrieval with autoencoders</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#optional-cheap-image-morphing"><span class="nav-number">10.</span> <span class="nav-text">Optional: Cheap image morphing</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">268</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">46</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zhangruochi" title="GitHub  https:&#x2F;&#x2F;github.com&#x2F;zhangruochi" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zrc720@gmail.com" title="E-Mail  mailto:zrc720@gmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.healthinformaticslab.org/" title="http:&#x2F;&#x2F;www.healthinformaticslab.org" rel="noopener" target="_blank">HILab</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.shihaizhou.com/" title="http:&#x2F;&#x2F;www.shihaizhou.com" rel="noopener" target="_blank">Rose</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/cherish_CX/" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;cherish_CX&#x2F;" rel="noopener" target="_blank">Chunxia</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019  
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>














  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
      appKey     : 'GL6JvT9DgGxqYrY5Vj6bXVuv',
      placeholder: "Thank you for your reply",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'en' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
