<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhangruochi.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="This is the lecture note of Upitt course INFSCI 2160 DATA MINING">
<meta property="og:type" content="article">
<meta property="og:title" content="INFSCI 2160 DATA MINING(PART ONE)">
<meta property="og:url" content="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:description" content="This is the lecture note of Upitt course INFSCI 2160 DATA MINING">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/29AF0C1A006683910109C1BA949541C4.png">
<meta property="og:image" content="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/83DD4C0B5649752C8A1EAE6A78058344.png">
<meta property="og:image" content="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/9784E656D9E9D2613CF3D07CA064D3AA.jpg">
<meta property="og:image" content="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/CA80BF1FC8D3AA010CDDFE5A646489BF.png">
<meta property="og:image" content="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/BFE33EA434D8355137098AEF9241AEEC.png">
<meta property="og:image" content="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/3FB3E0B341C2DD82395CB962430B6E34.png">
<meta property="og:image" content="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/youden.png">
<meta property="og:image" content="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/IMG_77C6064B4F18.png">
<meta property="og:image" content="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/BEC28A552A36163DACC7817A18999E6B.png">
<meta property="og:image" content="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/8227ABDFF7CF20D89F91BFAAE7C2C7F8.png">
<meta property="article:published_time" content="2019-02-18T05:11:34.000Z">
<meta property="article:modified_time" content="2019-07-05T10:09:09.000Z">
<meta property="article:author" content="Ruochi Zhang">
<meta property="article:tag" content="Course Note">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/29AF0C1A006683910109C1BA949541C4.png">

<link rel="canonical" href="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>INFSCI 2160 DATA MINING(PART ONE) | RUOCHI.AI</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">RUOCHI.AI</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          INFSCI 2160 DATA MINING(PART ONE)
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-02-18 13:11:34" itemprop="dateCreated datePublished" datetime="2019-02-18T13:11:34+08:00">2019-02-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-07-05 18:09:09" itemprop="dateModified" datetime="2019-07-05T18:09:09+08:00">2019-07-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Data-Mining/" itemprop="url" rel="index"><span itemprop="name">Data Mining</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
            <div class="post-description">This is the lecture note of Upitt course INFSCI 2160 DATA MINING</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><strong>Reference from some lecture slides of INFSCI 2160 DATA MINING lectured by Matt Berezo</strong></p>
<h2 id="introduction">Introduction</h2>
<ol type="1">
<li><p>What is Artificial Intelligence? <img src="29AF0C1A006683910109C1BA949541C4.png" alt="Screen Shot 2019-02-18 at 13.13.28.png" /> The goal of machine learning/AI/data mining is to develop an algorithm that performs well on new, <strong>unseen inputs</strong>. The ability to perform well on previously unobserved inputs is called <strong>generalization</strong></p></li>
<li><p>Data Mining process <img src="83DD4C0B5649752C8A1EAE6A78058344.png" alt="Screen Shot 2019-02-18 at 13.15.58.png" /></p></li>
</ol>
<ul>
<li><strong>Business undertanding</strong> is important</li>
<li><strong>Data understranding</strong> is important</li>
</ul>
<h2 id="regression">Regression</h2>
<ul>
<li>Simple linear regression involves 2 variables:
<ul>
<li>A predictor variable, x</li>
<li>A response variable, y</li>
</ul></li>
</ul>
<p><span class="math display">\[\hat{y_{i}} = \hat{\alpha} + \hat{\beta_{i}}X_{i}\]</span> - <span class="math inline">\(\hat{y_{i}}\)</span> = Estimated prediction of y - <span class="math inline">\(\hat{\alpha}\)</span> = Intercept - <span class="math inline">\(\hat{\beta_{i}}\)</span> = coefficient/parameter</p>
<p><strong>Goal</strong>: Obtain coefficient estimates that the linear model fits the available data well, and will also perform well (generalize) on unseen data</p>
<h3 id="the-least-square-approach">The least square approach</h3>
<p><span class="math display">\[\begin{align*}
&amp; \hat{\beta_{1}} = \frac{\sum_{i=1}^{n}{(x_{i}-\bar{x})(y_{i}-\bar{y})}}{\sum_{i=1}^{n}{(x_{i} - \bar{x})^{2}}}\\
&amp; \hat{\beta_{0}} = \bar{y} - \hat{\beta_{1}}\bar{x}
\end{align*}
\]</span></p>
<h3 id="coefficient-accuracy">Coefficient Accuracy</h3>
<p>We can compute the standard error of our coefficients</p>
<h4 id="what-is-se-standaed-error">what is SE (standaed error)?</h4>
<blockquote>
<p>If the purpose is <strong>Descriptive</strong>, use standard Deviation; if the purpose is <strong>Estimation</strong>, use standard Error.</p>
</blockquote>
<p>å¾ˆå®¹æ˜“æ··æ·†,æˆ‘ä»¬æ‹¿åˆ°ä¸€ä¸ªæ ·æœ¬,å¯¹æ ·æœ¬è§‚å¯Ÿå€¼ç¦»æ•£ç¨‹åº¦çš„é‡åŒ–æ˜¯<span class="math inline">\(SD:sd(x)\)</span>; è€Œæˆ‘ä»¬å¯ä»¥ä»å¾ˆå¤šä¸ªæ ·æœ¬ä¸­å¾—åˆ°å¾ˆå¤šä¸ªå‡å€¼ï¼Œè¿™äº›å‡å€¼çš„ç¦»æ•£åº¦ç”¨SEæ¥é‡åŒ–,ä¹Ÿå°±æ˜¯<span class="math inline">\(SE=sd(\bar{x})\)</span></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## generate 1000 sample with sample size 100</span></span><br><span class="line">a = sapply(<span class="built_in">rep</span>(<span class="number">100</span>, <span class="number">1000</span>), rnorm)</span><br><span class="line">a.mean = colMeans(a)</span><br><span class="line"><span class="comment">## estimate SEM by simulation</span></span><br><span class="line">sd(a.mean)</span><br><span class="line"></span><br><span class="line"><span class="comment">## estimate SEM by sigma/sqrt(n), sigma = 1</span></span><br><span class="line"><span class="number">1</span>/<span class="built_in">sqrt</span>(<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## estimate SEM by sample 1</span></span><br><span class="line">sd(a[, <span class="number">1</span>])/<span class="built_in">sqrt</span>(<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p><strong>We have established that the average of <span class="math inline">\(\hat{\mu}\)</span> over many data sets will be very close to <span class="math inline">\(\mu\)</span>, but that a single estimate <span class="math inline">\(\hat{\mu}\)</span> may be a substantial underestimate or overestimate of <span class="math inline">\(\mu\)</span>. How far off will that single estimate of <span class="math inline">\(\hat{\mu}\)</span> be?</strong></p>
<p><span class="math display">\[\begin{align*}
&amp; SE(\hat{\beta_{0}})^{2} = \sigma^{2}\lbrack\frac{1}{n} + \frac{\bar{x}^{2}}{\sum_{i=1}^{n}{(x_{i} - \bar{x})^{2}}}\rbrack\\
&amp; SE(\hat{\beta_{1}})^{2} = \frac{\sigma^{2}}{\sum_{i=1}^{n}{(x_{i} - \bar{x})^{2}}}\\
\end{align*}\]</span></p>
<p>When we get the SE of parameters, we can calculate the 95% confidence interval</p>
<p><span class="math display">\[\hat{B_{i}} = +/- 2 * SE(\hat{B_{i}})\]</span></p>
<p>Standard errors can also be used to perform <strong>hypothesis tests</strong> on the coefficients. The most common hypothesis test involves testing the null hypothesis</p>
<ul>
<li><strong>Null hypothesis(H0)</strong>: there is <strong>no</strong> relationship between x and y</li>
<li><strong>Alternative hypothesis(Ha)</strong>: there is a relationship between x and y</li>
</ul>
<p>Mathematically, this corresponds to testing <span class="math display">\[\begin{align*}
H_{0}: \beta_{1} = 0 \\
H_{a}: \beta_{1} \ne 0 \\
\end{align*}\]</span></p>
<p>To test the null hypothesis, we need to determine whether <span class="math inline">\(\hat{Î²_{1}}\)</span>, our estimate for <span class="math inline">\(\hat{Î²_{1}}\)</span> , is sufficiently far from zero that we can be confident that <span class="math inline">\(\hat{Î²_{1}}\)</span> is non-zero</p>
<p><span class="math display">\[t = \frac{\hat{Î²_{1}} - 0}{SE(\hat{\beta_{1}})}\]</span></p>
<h3 id="model-accuracy">Model Accuracy</h3>
<ul>
<li><p>RSS: Residual Sum of Squares <span class="math display">\[e_{1}^{2} + e_{2}^{2} + e_{3}^{2} + .....\]</span></p></li>
<li><p>RSE: Residual standard error <span class="math display">\[\sqrt\frac{RSS}{(N-2)}\]</span></p></li>
<li><p>R squared How much better does your model do than simply using the mean, in terms of SSE? <span class="math display">\[R^{2} = 1 - (\frac{\sum{(y_{i} - \hat{y}_{i})^{2}}}{\sum{(y_{i} - \overline{y}_{i})^{2}}})\]</span></p>
<ul>
<li>R-square takes form of a proportion and gives a value between 0 and 1 (1 = perfect model)</li>
</ul></li>
</ul>
<h3 id="multiple-linear-regression">Multiple Linear Regression</h3>
<ul>
<li>F-stat If the F-stat is larger than 1 and the p-value is &lt;= 0.05, we can determine that our predictors and model have a relationship with the response variable</li>
</ul>
<p><span class="math display">\[\frac{TSS - RSS}{p} / \frac{RSS}{n-p-1}\]</span> - Where p = our number of predictors - N = number of observations</p>
<figure>
<img src="9784E656D9E9D2613CF3D07CA064D3AA.jpg" alt="IMG_0310.jpg" /><figcaption aria-hidden="true">IMG_0310.jpg</figcaption>
</figure>
<ul>
<li>R-squared <span class="math display">\[R^{2} = 1 - \frac{\sum{(y_{i} - \hat{y}_{i})^{2}} / (n-d-1)}{\sum{(y_{i} - \overline{y}_{i})^{2}}/(n-d)}\]</span></li>
</ul>
<h3 id="feature-selection">Feature Selection</h3>
<h4 id="stepwise-procedures">Stepwise Procedures</h4>
<ul>
<li>Backward Elimination This is the simplest of all variable selection procedures and can be easily implemented without special software. In situations where there is a complex hierarchy, backward elimination can be run manually while taking account of what variables are eligible for removal.
<ol type="1">
<li>Start with all the predictors in the model</li>
<li>Remove the predictor with highest p-value greater than <span class="math inline">\(\alpha\)</span></li>
<li>Refit the model and goto 2</li>
<li>Stop when all p-values are less than <span class="math inline">\(\alpha\)</span></li>
</ol></li>
<li>Forward Selection This just reverses the backward method.
<ol type="1">
<li>Start with no variables in the model.</li>
<li>For all predictors not in the model, check their p-value if they are added to the model. Choose the one with lowest p-value less than Î±crit .</li>
<li>Continue until no new predictors can be added.</li>
</ol></li>
</ul>
<h4 id="ridge-regression-i.e.-l2-norm-regulizar">Ridge regression (i.e., L2 norm regulizar)</h4>
<p>Ridge looks to minimize: <span class="math display">\[RSS + \lambda\sum_{j=1}^{p}{\beta_{j}^{2}}\]</span></p>
<ul>
<li><span class="math inline">\(\lambda\)</span> is a tuning parameter</li>
</ul>
<h3 id="bias-vs.-variance-trade-off">Bias vs. Variance Trade-off</h3>
<p>Ideally, we want to derive a model that has low bias, low variance, and low MSE on test data <img src="CA80BF1FC8D3AA010CDDFE5A646489BF.png" alt="Screen Shot 2019-02-18 at 20.01.09.png" /></p>
<h3 id="local-polynomial-regression">Local Polynomial Regression</h3>
<ul>
<li>The fitted value changes with x in a nonparametric manner</li>
<li>Define a weight function so that only values within a smoothing window [ğ‘¥0 - h(ğ‘¥0 ), ğ‘¥0 + h(ğ‘¥0 )] will be considered in the estimate of <span class="math inline">\(\hat{y}\)</span></li>
</ul>
<h3 id="model-performance">Model Performance</h3>
<h3 id="cross-validation">Cross-validation</h3>
<p>The goal of cross-validation is to test the modelâ€™s ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias[6] and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).</p>
<h4 id="advantages-of-loocv">Advantages of LOOCV</h4>
<p>Advantages of LOOCV: - Works well on small datasets - Meticulously tests the data</p>
<p>Disadvantages of LOOCV: - Computationally expensive on â€œbig dataâ€ sets - Can result in high variability since model is only tested on one observation</p>
<h4 id="overfitting">Overfitting</h4>
<ul>
<li>Use cross-validation</li>
<li>Ensemble/combine models together</li>
<li>Use regularization techniques to penalize models that are too complex</li>
</ul>
<h3 id="non-parametric-methods">Non-parametric Methods</h3>
<p>Advantages of Non-parametric Methods: - Do not assume an explicit form of f(x), so the model is more "flexible"</p>
<p>Disadvantages of Non-Parametric Methods: - Often are more complex and thus more difficult to interpret</p>
<h4 id="k-nearest-neighbors">K Nearest-Neighbors</h4>
<ul>
<li>KNN is a <strong>non-parametric method</strong>, vs. linear and logistic regression which are parametric approaches since they assume a linear functional form for f(x) <img src="BFE33EA434D8355137098AEF9241AEEC.png" alt="Screen Shot 2019-02-18 at 22.26.44.png" /></li>
</ul>
<h3 id="accuracy-and-error-rate">Accuracy and Error Rate</h3>
<p><img src="3FB3E0B341C2DD82395CB962430B6E34.png" alt="Screen Shot 2019-02-19 at 00.26.12.png" /> <img src="youden.png" alt="youden.png" /> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">scores = np.array([<span class="number">0.1</span>, <span class="number">0.4</span>, <span class="number">0.35</span>, <span class="number">0.8</span>])</span><br><span class="line">fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fpr</span><br><span class="line">array([<span class="number">0.</span> , <span class="number">0.</span> , <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">1.</span> ])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tpr</span><br><span class="line">array([<span class="number">0.</span> , <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">1.</span> , <span class="number">1.</span> ])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>thresholds</span><br><span class="line">array([<span class="number">1.8</span> , <span class="number">0.8</span> , <span class="number">0.4</span> , <span class="number">0.35</span>, <span class="number">0.1</span> ])</span><br></pre></td></tr></table></figure></p>
<h3 id="classification">Classification</h3>
<h4 id="naÃ¯ve-bayes">NaÃ¯ve Bayes</h4>
<ul>
<li>a NaÃ¯ve Bayes classifier assumes independence between features</li>
<li>NaÃ¯ve Bayes assumes that the continuous variables are normally distributed</li>
<li>For continuous random variables, probabilities are areas under the curve <img src="IMG_77C6064B4F18.png" alt="IMG_77C6064B4F18.png" /></li>
</ul>
<h4 id="decision-trees">Decision Trees</h4>
<p>Constructing Decision Trees for Regression 1. First, we divide the predictor space into distinct and non-overlapping regions (ğ‘…1, ğ‘…2,ğ‘…3 ... ğ‘…ğ‘›) 2. To make a prediction, we typically use the mean of the training data in the region to which it belongs</p>
<p><strong>How do we construct R1 and R2?</strong> The goal is to find regions that minimize the residual sum of squares (RSS)</p>
<p>Decision trees can get too complex, memorize the training data, and overfit on test data - It is advised to first build a very large tree and then <strong>prune</strong> it back to obtain a subtree - Given a subtree, we can estimate the test error rate using cross-validation - <strong>Cost complexity pruning</strong> i.e., <strong>weakest link pruning</strong> gives us the most efficient way to choose our subset of trees</p>
<h4 id="decision-trees-advantages-and-disadvantages">Decision Trees Advantages and Disadvantages</h4>
<p>Advantages: - Trees are easy to explain and are intuitive - Trees can be displayed graphically and are easy to interpret - Trees can handle qualitative predictors without dummy variables</p>
<p>Disadvantages: - Trees to not usually have the same level of predictive accuracy as other regression and classification methods - Trees can be non-robust, i.e., a small change in the data can cause a large change in the tree</p>
<h4 id="bagging-and-random-forests">Bagging and Random Forests</h4>
<p><strong>Bootstrap aggregation</strong>, also known as <strong>bagging</strong>, is a procedure of reducing the variance of a statistical learning method - This is a good way to reduce varianceâ†’by taking many training sets from the population and build separate learning methods using each set - We can then calculate f1,f2,f3... and average them in order to obtain a low-variance statistical model - We can do this by bootstrapping, or taking repeated random samples from the training set</p>
<p><strong>Ensemble learning</strong> is a machine learning paradigm where multiple learners are trained to solve the same problem</p>
<p><strong>Random forests</strong> provide an improvement over bagged trees by decorrelating them - Like bagging, decision trees are made on bootstrapped training samples - Random forests are an <strong>ensemble</strong> method for decision trees - The difference is, each time a split in the tree is considered, a random sample of predictors is chosen as split candidates from the full set of predictors. So, at each split of the tree, the algorithm canâ€™t even consider a majority of the predictors</p>
<h3 id="support-vector-machines">Support Vector Machines</h3>
<ul>
<li>SVMâ€™s use a classifying tool called <strong>maximum margin classifier</strong></li>
<li>maximum margin classifiers canâ€™t be applied to most datasets because they require the classes to be <strong>separated by a linear boundary</strong></li>
<li>Support vector classifiers are an extension of maximum margin classifiers that can be applied to a broader range of datasets</li>
</ul>
<h4 id="hyperplane">hyperplane</h4>
<p>A hyperplane is a flat subspace in p-dimensional space with p â€“ 1 dimensions <img src="BEC28A552A36163DACC7817A18999E6B.png" alt="Screen Shot 2019-02-19 at 02.46.26.png" /></p>
<h4 id="parameters">parameters</h4>
<ul>
<li>C = a nonnegative tuning parameter
<ul>
<li>C can be thought of as a budget for the amount the margin can be violated by n observations. If C = 0, there is no budget for violations to the margin</li>
<li>large C, Overfitting</li>
<li>small C, underfitting</li>
</ul></li>
</ul>
<h4 id="what-if-the-decision-boundary-for-the-two-classes-is-not-linear">What if the decision boundary for the two classes is not linear?</h4>
<ul>
<li>enlarging the feature space with kernels</li>
<li>A kernel is a function that quantifies the similarity between two observations</li>
</ul>
<h3 id="multinomial-logistic-regression">Multinomial Logistic Regression</h3>
<ul>
<li>Similar to binary logistic regression, all probabilities in the output will sum to 1</li>
<li>This is just an extension of the same math from logistic regression</li>
</ul>
<h4 id="drawbacks">Drawbacks</h4>
<ul>
<li>Models involve many parameters, which makes their interpretation tedious</li>
<li>Maximum-likelihood estimation can encounter numerical problems if the data is separable and if the predicted probabilities are close to either 0 or 1</li>
</ul>
<h3 id="xgboost">XGBoost</h3>
<h4 id="overview">overview</h4>
<p>Advantages of XGBoost:</p>
<ul>
<li>Scalability: XGBoost system runs 10x faster than existing popular solutions on a single machine</li>
<li>XGBoost accepts null values: users donâ€™t have to impute missing values, drop records, etc.</li>
<li>Less time spent on feature selection and more time spent on hyperparametric tuning</li>
</ul>
<p>Typically, one tree is not as strong as an ensemble/combination of other trees. XGBoost uses an ensemble method to gather information from other trees.</p>
<h4 id="objective-function-and-regularization">Objective Function and Regularization</h4>
<p>The additive function fixes what we have already learned, and adds one new tree at a time <img src="8227ABDFF7CF20D89F91BFAAE7C2C7F8.png" alt="Screen Shot 2019-02-19 at 04.46.06.png" /></p>
<p>But how do we choose which tree we want at each step? &gt; We pick the one that optimizes our objective function! This is known as an <strong>additive function</strong></p>
<h4 id="objective-functions">Objective Functions</h4>
<ul>
<li>Linear: Continuous numeric prediction</li>
<li>Binary: logistic,binary classification</li>
<li>Multi:softmax: multiclassification</li>
</ul>
<h4 id="tree-boosting-parameters">Tree Boosting Parameters</h4>
<p><strong>Reference from <a target="_blank" rel="noopener" href="https://www.cnblogs.com/sarahp/p/6900572.html">https://www.cnblogs.com/sarahp/p/6900572.html</a></strong></p>
<ul>
<li><strong>Eta</strong> (i.e., learning rate): Step shrinkage use in update to prevent overfitting. After each boosting step, we can get the weights of new features. Eta shrinks the weights to make the boosting process more conservative</li>
<li>Gamma: Minimum loss reduction required to make a further partition on a leaf node of a tree. Larger gamma = more conservative model (è¿™ä¸ªæŒ‡å®šäº†ä¸€ä¸ªç»“ç‚¹è¢«åˆ†å‰²æ—¶ï¼Œæ‰€éœ€è¦çš„æœ€å°æŸå¤±å‡½æ•°å‡å°çš„å¤§å°)</li>
<li>Max depth: Maximum depth of a tree. Increasing this value will make the model more complex (æ ‘çš„æœ€å¤§æ·±åº¦ï¼Œå€¼è¶Šå¤§ï¼Œæ ‘è¶Šå¤æ‚)</li>
<li>Minimum child weight: Minimum sum of instance weight needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than this set parameter, the building process will stop partitioning. Larger weight = more conservative model (å®šä¹‰äº†ä¸€ä¸ªå­é›†çš„æ‰€æœ‰è§‚å¯Ÿå€¼çš„æœ€å°æƒé‡å’Œ)</li>
<li>Subsample: A subsample ratio of the training instances. Setting to 0.5 would make XGBoost randomly sample half of the training data prior to growing trees and will help prevent overfitting (æ ·æœ¬çš„é‡‡æ ·ç‡ï¼Œå¦‚æœè®¾ç½®æˆ0.5ï¼Œé‚£ä¹ˆXgboostä¼šéšæœºé€‰æ‹©ä¸€èˆ¬çš„æ ·æœ¬ä½œä¸ºè®­ç»ƒé›†)</li>
<li>Column sample by tree: Subsample ratio of columns when constructing a tree</li>
<li>Column sample by level: Subsample ratio of columns for each level of the tree</li>
<li>Column sample by node: Subsample ratio of columns for each node (split)</li>
<li>Lambda: L2 regularization</li>
<li>Alpha: L1 regularization</li>
<li>Scale positive weight: Control the balance of positive and negative weights</li>
</ul>
<h3 id="review">REVIEW</h3>
<h4 id="what-is-the-difference-between-boost-ensemble-bootstrap-and-bagging">What is the difference between boost, ensemble, bootstrap and bagging?</h4>
<p><strong>Reference from <a target="_blank" rel="noopener" href="https://www.quora.com/What-is-the-difference-between-boost-ensemble-bootstrap-and-bagging">https://www.quora.com/What-is-the-difference-between-boost-ensemble-bootstrap-and-bagging</a></strong></p>
<ul>
<li>Boosting is the idea of training iteratively the same â€œweakâ€ classifier, so that at each iteration, the i-th classifier is supposed to correct the mistakes made by the previous classifier (i-1). It is done by weighting more the misclassified observations.</li>
<li>The final classifier is calculated by a weighted mean of all the â€œweakâ€ classifiers, the weights being close to the accuracies calculated for each classifier.</li>
<li>Ensembling is quite general and encompasses simple methods like Averaging, and more complicated ones like Boosting, Bagging, Stacking, etc.</li>
<li>Bootstrapping means taking a sample of a population by drawing with replacement. It is one of the main ideas behind Bagging (which stands for Bootstrap AGGregatING).</li>
<li>Bagging means training the same classifier on different subsets (that may be overlapping) of one dataset. You do so with bootstrap.</li>
</ul>
<h4 id="rf-vs-xgboost">RF vs XGBoost</h4>
<p><strong>Reference from <a target="_blank" rel="noopener" href="https://www.cnblogs.com/sarahp/p/6900572.html">https://www.cnblogs.com/sarahp/p/6900572.html</a></strong></p>
<ul>
<li>RF use bagging:
<ul>
<li>ç§é›†æˆå­¦ä¹ ç®—æ³•ï¼ŒåŸºäºbootstrap sampling è‡ªåŠ©é‡‡æ ·æ³•ï¼Œé‡å¤æ€§æœ‰æ”¾å›çš„éšæœºé‡‡ç”¨éƒ¨åˆ†æ ·æœ¬è¿›è¡Œè®­ç»ƒæœ€åå†å°†ç»“æœ voting æˆ–è€… averaging</li>
<li>å®ƒæ˜¯å¹¶è¡Œå¼ç®—æ³•ï¼Œå› ä¸ºä¸åŒåŸºå­¦ä¹ å™¨æ˜¯ç‹¬ç«‹</li>
<li>è®­ç»ƒä¸€ä¸ªbaggingé›†æˆå­¦ä¹ å™¨æ—¶é—´å¤æ‚åº¦ä¸åŸºå­¦ä¹ å™¨åŒé˜¶ï¼ˆnå€ï¼Œnä¸ºåŸºå­¦ä¹ å™¨ä¸ªæ•°ï¼‰ã€‚</li>
<li>baggingå¯ä»¥ç”¨äºäºŒåˆ†ç±»ï¼å¤šåˆ†ç±»ï¼å›å½’</li>
<li>æ¯ä¸ªåŸºå­¦ä¹ å™¨çš„æœªç”¨ä½œè®­ç»ƒæ ·æœ¬å¯ç”¨æ¥åšåŒ…å¤–ä¼°è®¡ï¼Œè¯„ä»·æ³›åŒ–æ€§èƒ½ã€‚</li>
<li>baggingä¸»è¦å…³æ³¨é™ä½<strong>æ–¹å·®</strong></li>
<li>ä¸¤ä¸ªæ­¥éª¤ 1. æŠ½æ ·è®­ç»ƒï¼ˆé‡‡æ ·æ ·æœ¬ï¼Œé‡‡æ ·ç‰¹å¾ï¼‰ 2 èåˆ</li>
</ul></li>
<li>XGBoost use boosting(Gradient Boosting Decision Tree):
<ul>
<li>gbdtçš„åŸºæœ¬åŸç†æ˜¯boost é‡Œé¢çš„ boosting treeï¼ˆæå‡æ ‘ï¼‰ï¼Œå¹¶ä½¿ç”¨ gradient boostã€‚</li>
<li>Gradient Boostingæ˜¯ä¸€ç§Boostingçš„æ–¹æ³•ï¼Œå…¶ä¸ä¼ ç»Ÿçš„Boostingçš„åŒºåˆ«æ˜¯ï¼Œæ¯ä¸€æ¬¡çš„è®¡ç®—æ˜¯ä¸ºäº†å‡å°‘ä¸Šä¸€æ¬¡çš„æ®‹å·®(residual)ï¼Œè€Œä¸ºäº†æ¶ˆé™¤æ®‹å·®ï¼Œå¯ä»¥åœ¨æ®‹å·®å‡å°‘çš„æ¢¯åº¦(Gradient)æ–¹å‘ä¸Šå»ºç«‹ä¸€ä¸ªæ–°çš„æ¨¡å‹ã€‚æ‰€ä»¥è¯´ï¼Œåœ¨Gradient Boostingä¸­ï¼Œæ¯ä¸ªæ–°çš„æ¨¡å‹çš„å»ºç«‹æ˜¯ä¸ºäº†ä½¿å¾—ä¹‹å‰æ¨¡å‹çš„æ®‹å·®å¾€æ¢¯åº¦æ–¹å‘å‡å°‘ï¼Œä¸ä¼ ç»ŸBoostingå¯¹æ­£ç¡®ã€é”™è¯¯æ ·æœ¬è¿›è¡ŒåŠ æƒæœ‰ç€å¾ˆå¤§çš„åŒºåˆ«ã€‚è¿™ä¸ªæ¢¯åº¦ä»£è¡¨ä¸Šä¸€è½®å­¦ä¹ å™¨æŸå¤±å‡½æ•°å¯¹é¢„æµ‹å€¼æ±‚å¯¼ã€‚</li>
<li>ä¸Boosting Treeçš„åŒºåˆ«ï¼šBoosting Treeçš„é€‚åˆäºæŸå¤±å‡½æ•°ä¸ºå¹³æ–¹æŸå¤±æˆ–è€…æŒ‡æ•°æŸå¤±ã€‚è€ŒGradient Boostingé€‚åˆå„ç±»æŸå¤±å‡½æ•°ï¼ˆæŸå¤±å‡½æ•°ä¸ºï¼šå¹³æ–¹æŸå¤±åˆ™ç›¸å½“äºBoosting Treeæ‹Ÿåˆæ®‹å·®ã€æŸå¤±å‡½æ•°ä¸ºï¼šä½¿ç”¨æŒ‡æ•°æŸå¤±åˆ™å¯ä»¥è¿‘ä¼¼äºAdaboostï¼Œä½†æ ‘æ˜¯å›å½’æ ‘ï¼‰</li>
</ul></li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Course-Note/" rel="tag"># Course Note</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/Machine-Learning-Andraw-Ng/2019/02/15/" rel="prev" title="Machine Learning (Andrew Ng)">
      <i class="fa fa-chevron-left"></i> Machine Learning (Andrew Ng)
    </a></div>
      <div class="post-nav-item">
    <a href="/Linear-models-Optimization/2019/02/22/" rel="next" title="Linear models, Optimization">
      Linear models, Optimization <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#regression"><span class="nav-number">2.</span> <span class="nav-text">Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#the-least-square-approach"><span class="nav-number">2.1.</span> <span class="nav-text">The least square approach</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#coefficient-accuracy"><span class="nav-number">2.2.</span> <span class="nav-text">Coefficient Accuracy</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#what-is-se-standaed-error"><span class="nav-number">2.2.1.</span> <span class="nav-text">what is SE (standaed error)?</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#model-accuracy"><span class="nav-number">2.3.</span> <span class="nav-text">Model Accuracy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#multiple-linear-regression"><span class="nav-number">2.4.</span> <span class="nav-text">Multiple Linear Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#feature-selection"><span class="nav-number">2.5.</span> <span class="nav-text">Feature Selection</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#stepwise-procedures"><span class="nav-number">2.5.1.</span> <span class="nav-text">Stepwise Procedures</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ridge-regression-i.e.-l2-norm-regulizar"><span class="nav-number">2.5.2.</span> <span class="nav-text">Ridge regression (i.e., L2 norm regulizar)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bias-vs.-variance-trade-off"><span class="nav-number">2.6.</span> <span class="nav-text">Bias vs. Variance Trade-off</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#local-polynomial-regression"><span class="nav-number">2.7.</span> <span class="nav-text">Local Polynomial Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#model-performance"><span class="nav-number">2.8.</span> <span class="nav-text">Model Performance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cross-validation"><span class="nav-number">2.9.</span> <span class="nav-text">Cross-validation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#advantages-of-loocv"><span class="nav-number">2.9.1.</span> <span class="nav-text">Advantages of LOOCV</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#overfitting"><span class="nav-number">2.9.2.</span> <span class="nav-text">Overfitting</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#non-parametric-methods"><span class="nav-number">2.10.</span> <span class="nav-text">Non-parametric Methods</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#k-nearest-neighbors"><span class="nav-number">2.10.1.</span> <span class="nav-text">K Nearest-Neighbors</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#accuracy-and-error-rate"><span class="nav-number">2.11.</span> <span class="nav-text">Accuracy and Error Rate</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#classification"><span class="nav-number">2.12.</span> <span class="nav-text">Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#na%C3%AFve-bayes"><span class="nav-number">2.12.1.</span> <span class="nav-text">NaÃ¯ve Bayes</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#decision-trees"><span class="nav-number">2.12.2.</span> <span class="nav-text">Decision Trees</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#decision-trees-advantages-and-disadvantages"><span class="nav-number">2.12.3.</span> <span class="nav-text">Decision Trees Advantages and Disadvantages</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bagging-and-random-forests"><span class="nav-number">2.12.4.</span> <span class="nav-text">Bagging and Random Forests</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#support-vector-machines"><span class="nav-number">2.13.</span> <span class="nav-text">Support Vector Machines</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#hyperplane"><span class="nav-number">2.13.1.</span> <span class="nav-text">hyperplane</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#parameters"><span class="nav-number">2.13.2.</span> <span class="nav-text">parameters</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#what-if-the-decision-boundary-for-the-two-classes-is-not-linear"><span class="nav-number">2.13.3.</span> <span class="nav-text">What if the decision boundary for the two classes is not linear?</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#multinomial-logistic-regression"><span class="nav-number">2.14.</span> <span class="nav-text">Multinomial Logistic Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#drawbacks"><span class="nav-number">2.14.1.</span> <span class="nav-text">Drawbacks</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#xgboost"><span class="nav-number">2.15.</span> <span class="nav-text">XGBoost</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#overview"><span class="nav-number">2.15.1.</span> <span class="nav-text">overview</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#objective-function-and-regularization"><span class="nav-number">2.15.2.</span> <span class="nav-text">Objective Function and Regularization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#objective-functions"><span class="nav-number">2.15.3.</span> <span class="nav-text">Objective Functions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tree-boosting-parameters"><span class="nav-number">2.15.4.</span> <span class="nav-text">Tree Boosting Parameters</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#review"><span class="nav-number">2.16.</span> <span class="nav-text">REVIEW</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#what-is-the-difference-between-boost-ensemble-bootstrap-and-bagging"><span class="nav-number">2.16.1.</span> <span class="nav-text">What is the difference between boost, ensemble, bootstrap and bagging?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#rf-vs-xgboost"><span class="nav-number">2.16.2.</span> <span class="nav-text">RF vs XGBoost</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">218</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">46</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zhangruochi" title="GitHub â†’ https:&#x2F;&#x2F;github.com&#x2F;zhangruochi" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zrc720@gmail.com" title="E-Mail â†’ mailto:zrc720@gmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.healthinformaticslab.org/" title="http:&#x2F;&#x2F;www.healthinformaticslab.org" rel="noopener" target="_blank">HILab</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.shihaizhou.com/" title="http:&#x2F;&#x2F;www.shihaizhou.com" rel="noopener" target="_blank">Rose</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/cherish_CX/" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;cherish_CX&#x2F;" rel="noopener" target="_blank">Chunxia</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 â€“ 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
      appKey     : 'GL6JvT9DgGxqYrY5Vj6bXVuv',
      placeholder: "Thank you for your reply",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'en' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
