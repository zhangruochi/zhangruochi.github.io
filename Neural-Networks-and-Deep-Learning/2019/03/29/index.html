<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhangruochi.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="This is course note of the deep learning specialization at lectured by Andrew Ng.">
<meta property="og:type" content="article">
<meta property="og:title" content="Neural Networks and Deep Learning">
<meta property="og:url" content="https://zhangruochi.com/Neural-Networks-and-Deep-Learning/2019/03/29/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:description" content="This is course note of the deep learning specialization at lectured by Andrew Ng.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zhangruochi.com/Neural-Networks-and-Deep-Learning/2019/03/29/images/12.jpg">
<meta property="og:image" content="https://zhangruochi.com/Neural-Networks-and-Deep-Learning/2019/03/29/images/13.png">
<meta property="og:image" content="https://zhangruochi.com/Neural-Networks-and-Deep-Learning/2019/03/29/images/11.png">
<meta property="og:image" content="https://zhangruochi.com/Neural-Networks-and-Deep-Learning/2019/03/29/images/14.png">
<meta property="og:image" content="https://zhangruochi.com/Neural-Networks-and-Deep-Learning/2019/03/29/images/02.png">
<meta property="og:image" content="https://zhangruochi.com/Neural-Networks-and-Deep-Learning/2019/03/29/images/03.png">
<meta property="og:image" content="https://zhangruochi.com/Neural-Networks-and-Deep-Learning/2019/03/29/images/04.png">
<meta property="og:image" content="https://zhangruochi.com/Neural-Networks-and-Deep-Learning/2019/03/29/images/09.png">
<meta property="og:image" content="https://zhangruochi.com/Neural-Networks-and-Deep-Learning/2019/03/29/images/05.png">
<meta property="og:image" content="https://zhangruochi.com/Neural-Networks-and-Deep-Learning/2019/03/29/images/06.png">
<meta property="og:image" content="https://zhangruochi.com/Neural-Networks-and-Deep-Learning/2019/03/29/images/07.png">
<meta property="og:image" content="https://zhangruochi.com/Neural-Networks-and-Deep-Learning/2019/03/29/images/10.png">
<meta property="og:image" content="https://zhangruochi.com/Neural-Networks-and-Deep-Learning/2019/03/29/images/08.png">
<meta property="article:published_time" content="2019-03-29T09:53:34.000Z">
<meta property="article:modified_time" content="2019-07-05T09:56:56.904Z">
<meta property="article:author" content="Ruochi Zhang">
<meta property="article:tag" content="Course Note">
<meta property="article:tag" content="Project">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhangruochi.com/Neural-Networks-and-Deep-Learning/2019/03/29/images/12.jpg">

<link rel="canonical" href="https://zhangruochi.com/Neural-Networks-and-Deep-Learning/2019/03/29/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Neural Networks and Deep Learning | RUOCHI.AI</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">RUOCHI.AI</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Neural-Networks-and-Deep-Learning/2019/03/29/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Neural Networks and Deep Learning
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-03-29 17:53:34" itemprop="dateCreated datePublished" datetime="2019-03-29T17:53:34+08:00">2019-03-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-07-05 17:56:56" itemprop="dateModified" datetime="2019-07-05T17:56:56+08:00">2019-07-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Neural-Networks-and-Deep-Learning/2019/03/29/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Neural-Networks-and-Deep-Learning/2019/03/29/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
            <div class="post-description">This is course note of the deep learning specialization at lectured by Andrew Ng.</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><strong>Reference from lecture slides of Andrew Ng and github repo from <a target="_blank" rel="noopener" href="https://github.com/mbadry1/DeepLearning.ai-Summary">DeepLearning.ai-Summary</a></strong></p>
<h2 id="table-of-contents">Table of contents</h2>
<ul>
<li><a href="#neural-networks-and-deep-learning">Neural Networks and Deep Learning</a>
<ul>
<li><a href="#table-of-contents">Table of contents</a></li>
<li><a href="#course-summary">Course summary</a></li>
<li><a href="#introduction-to-deep-learning">Introduction to deep learning</a>
<ul>
<li><a href="#what-is-a-neural-network-nn">What is a (Neural Network) NN?</a></li>
<li><a href="#supervised-learning-with-neural-networks">Supervised learning with neural networks</a></li>
<li><a href="#why-is-deep-learning-taking-off">Why is deep learning taking off?</a></li>
</ul></li>
<li><a href="#neural-networks-basics">Neural Networks Basics</a>
<ul>
<li><a href="#binary-classification">Binary classification</a></li>
<li><a href="#logistic-regression">Logistic regression</a></li>
<li><a href="#logistic-regression-cost-function">Logistic regression cost function</a></li>
<li><a href="#gradient-descent">Gradient Descent</a></li>
<li><a href="#derivatives">Derivatives</a></li>
<li><a href="#more-derivatives-examples">More Derivatives examples</a></li>
<li><a href="#computation-graph">Computation graph</a></li>
<li><a href="#derivatives-with-a-computation-graph">Derivatives with a Computation Graph</a></li>
<li><a href="#logistic-regression-gradient-descent">Logistic Regression Gradient Descent</a></li>
<li><a href="#gradient-descent-on-m-examples">Gradient Descent on m Examples</a></li>
<li><a href="#vectorization">Vectorization</a></li>
<li><a href="#vectorizing-logistic-regression">Vectorizing Logistic Regression</a></li>
<li><a href="#notes-on-python-and-numpy">Notes on Python and NumPy</a></li>
<li><a href="#general-notes">General Notes</a></li>
</ul></li>
<li><a href="#shallow-neural-networks">Shallow neural networks</a>
<ul>
<li><a href="#neural-networks-overview">Neural Networks Overview</a></li>
<li><a href="#neural-network-representation">Neural Network Representation</a></li>
<li><a href="#computing-a-neural-networks-output">Computing a Neural Network's Output</a></li>
<li><a href="#vectorizing-across-multiple-examples">Vectorizing across multiple examples</a></li>
<li><a href="#activation-functions">Activation functions</a></li>
<li><a href="#why-do-you-need-non-linear-activation-functions">Why do you need non-linear activation functions?</a></li>
<li><a href="#derivatives-of-activation-functions">Derivatives of activation functions</a></li>
<li><a href="#gradient-descent-for-neural-networks">Gradient descent for Neural Networks</a></li>
<li><a href="#random-initialization">Random Initialization</a></li>
</ul></li>
<li><a href="#deep-neural-networks">Deep Neural Networks</a>
<ul>
<li><a href="#deep-l-layer-neural-network">Deep L-layer neural network</a></li>
<li><a href="#forward-propagation-in-a-deep-network">Forward Propagation in a Deep Network</a></li>
<li><a href="#getting-your-matrix-dimensions-right">Getting your matrix dimensions right</a></li>
<li><a href="#why-deep-representations">Why deep representations?</a></li>
<li><a href="#building-blocks-of-deep-neural-networks">Building blocks of deep neural networks</a></li>
<li><a href="#forward-and-backward-propagation">Forward and Backward Propagation</a></li>
<li><a href="#parameters-vs-hyperparameters">Parameters vs Hyperparameters</a></li>
<li><a href="#what-does-this-have-to-do-with-the-brain">What does this have to do with the brain</a></li>
</ul></li>
<li><a href="#extra-ian-goodfellow-interview">Extra: Ian Goodfellow interview</a></li>
</ul></li>
</ul>
<h2 id="course-summary">Course summary</h2>
<p>Here are the course summary as its given on the course <a target="_blank" rel="noopener" href="https://www.coursera.org/learn/neural-networks-deep-learning">link</a>:</p>
<blockquote>
<p>If you want to break into cutting-edge AI, this course will help you do so. Deep learning engineers are highly sought after, and mastering deep learning will give you numerous new career opportunities. Deep learning is also a new "superpower" that will let you build AI systems that just weren't possible a few years ago.</p>
<p>In this course, you will learn the foundations of deep learning. When you finish this class, you will: - Understand the major technology trends driving Deep Learning - Be able to build, train and apply fully connected deep neural networks - Know how to implement efficient (vectorized) neural networks - Understand the key parameters in a neural network's architecture</p>
<p>This course also teaches you how Deep Learning actually works, rather than presenting only a cursory or surface-level description. So after completing it, you will be able to apply deep learning to a your own applications. If you are looking for a job in AI, after this course you will also be able to answer basic interview questions.</p>
</blockquote>
<h2 id="introduction-to-deep-learning">Introduction to deep learning</h2>
<blockquote>
<p>Be able to explain the major trends driving the rise of deep learning, and understand where and how it is applied today.</p>
</blockquote>
<h3 id="what-is-a-neural-network-nn">What is a (Neural Network) NN?</h3>
<ul>
<li>Single neuron == linear regression</li>
<li>Simple NN graph:
<ul>
<li><img src="images/12.jpg" /></li>
<li>Image taken from <a href="tutorialspoint.com">tutorialspoint.com</a></li>
</ul></li>
<li>RELU stands for rectified linear unit is the most popular activation function right now that makes deep NNs train faster now.</li>
<li>Hidden layers predicts connection between inputs automatically, thats what deep learning is good at.</li>
<li>Deep NN consists of more hidden layers (Deeper layers)
<ul>
<li><img src="images/13.png" /></li>
<li>Image taken from <a href="opennn.net">opennn.net</a></li>
</ul></li>
<li>Each Input will be connected to the hidden layer and the NN will decide the connections.</li>
<li>Supervised learning means we have the (X,Y) and we need to get the function that maps X to Y.</li>
</ul>
<h3 id="supervised-learning-with-neural-networks">Supervised learning with neural networks</h3>
<ul>
<li>Different types of neural networks for supervised learning which includes:
<ul>
<li>CNN or convolutional neural networks (Useful in computer vision)</li>
<li>RNN or Recurrent neural networks (Useful in Speech recognition or NLP)</li>
<li>Standard NN (Useful for Structured data)</li>
<li>Hybrid/custom NN or a Collection of NNs types</li>
</ul></li>
<li>Structured data is like the databases and tables.</li>
<li>Unstructured data is like images, video, audio, and text.</li>
<li>Structured data gives more money because companies relies on prediction on its big data.</li>
</ul>
<h3 id="why-is-deep-learning-taking-off">Why is deep learning taking off?</h3>
<ul>
<li>Deep learning is taking off for 3 reasons:
<ol type="1">
<li>Data:
<ul>
<li>Using this image we can conclude:
<ul>
<li><img src="images/11.png" /></li>
</ul></li>
<li>For small data NN can perform as Linear regression or SVM (Support vector machine)</li>
<li>For big data a small NN is better that SVM</li>
<li>For big data a big NN is better that a medium NN is better that small NN.</li>
<li>Hopefully we have a lot of data because the world is using the computer a little bit more
<ul>
<li>Mobiles</li>
<li>IOT (Internet of things)</li>
</ul></li>
</ul></li>
<li>Computation:
<ul>
<li>GPUs.</li>
<li>Powerful CPUs.</li>
<li>Distributed computing.</li>
<li>ASICs</li>
</ul></li>
<li>Algorithm:
<ol type="1">
<li>Creative algorithms has appeared that changed the way NN works.
<ul>
<li>For example using RELU function is so much better than using SIGMOID function in training a NN because it helps with the vanishing gradient problem.</li>
</ul></li>
</ol></li>
</ol>
â€‹</li>
</ul>
<h2 id="neural-networks-basics">Neural Networks Basics</h2>
<blockquote>
<p>Learn to set up a machine learning problem with a neural network mindset. Learn to use vectorization to speed up your models.</p>
</blockquote>
<h3 id="binary-classification">Binary classification</h3>
<ul>
<li>Mainly he is talking about how to do a logistic regression to make a binary classifier.
<ul>
<li><img src="images/14.png" title="fig:" alt="log" /></li>
<li>Image taken from <a target="_blank" rel="noopener" href="http://3.bp.blogspot.com">3.bp.blogspot.com</a></li>
</ul></li>
<li>He talked about an example of knowing if the current image contains a cat or not.</li>
<li>Here are some notations:
<ul>
<li><code>M is the number of training vectors</code></li>
<li><code>Nx is the size of the input vector</code></li>
<li><code>Ny is the size of the output vector</code></li>
<li><code>X(1) is the first input vector</code></li>
<li><code>Y(1) is the first output vector</code></li>
<li><code>X = [x(1) x(2).. x(M)]</code></li>
<li><code>Y = (y(1) y(2).. y(M))</code></li>
</ul></li>
<li>We will use python in this course.</li>
<li>In NumPy we can make matrices and make operations on them in a fast and reliable time.</li>
</ul>
<h3 id="logistic-regression">Logistic regression</h3>
<ul>
<li>Algorithm is used for classification algorithm of 2 classes.</li>
<li>Equations:
<ul>
<li>Simple equation: <code>y = wx + b</code></li>
<li>If x is a vector: <code>y = w(transpose)x + b</code></li>
<li>If we need y to be in between 0 and 1 (probability): <code>y = sigmoid(w(transpose)x + b)</code></li>
<li>In some notations this might be used: <code>y = sigmoid(w(transpose)x)</code>
<ul>
<li>While <code>b</code> is <code>w0</code> of <code>w</code> and we add <code>x0 = 1</code>. but we won't use this notation in the course (Andrew said that the first notation is better).</li>
</ul></li>
</ul></li>
<li>In binary classification <code>Y</code> has to be between <code>0</code> and <code>1</code>.</li>
<li>In the last equation <code>w</code> is a vector of <code>Nx</code> and <code>b</code> is a real number</li>
</ul>
<h3 id="logistic-regression-cost-function">Logistic regression cost function</h3>
<ul>
<li>First loss function would be the square root error: <code>L(y',y) = 1/2 (y' - y)^2</code>
<ul>
<li>But we won't use this notation because it leads us to optimization problem which is non convex, means it contains local optimum points.</li>
</ul></li>
<li>This is the function that we will use: <code>L(y',y) = - (y*log(y') + (1-y)*log(1-y'))</code></li>
<li>To explain the last function lets see:
<ul>
<li>if <code>y = 1</code> ==&gt; <code>L(y',1) = -log(y')</code> ==&gt; we want <code>y'</code> to be the largest ==&gt; <code>y</code>' biggest value is 1</li>
<li>if <code>y = 0</code> ==&gt; <code>L(y',0) = -log(1-y')</code> ==&gt; we want <code>1-y'</code> to be the largest ==&gt; <code>y'</code> to be smaller as possible because it can only has 1 value.</li>
</ul></li>
<li>Then the Cost function will be: <code>J(w,b) = (1/m) * Sum(L(y'[i],y[i]))</code></li>
<li>The loss function computes the error for a single training example; the cost function is the average of the loss functions of the entire training set.</li>
</ul>
<h3 id="gradient-descent">Gradient Descent</h3>
<ul>
<li><p>We want to predict <code>w</code> and <code>b</code> that minimize the cost function.</p></li>
<li><p>Our cost function is convex.</p></li>
<li><p>First we initialize <code>w</code> and <code>b</code> to 0,0 or initialize them to a random value in the convex function and then try to improve the values the reach minimum value.</p></li>
<li><p>In Logistic regression people always use 0,0 instead of random.</p></li>
<li><p>The gradient decent algorithm repeats: <code>w = w - alpha * dw</code> where alpha is the learning rate and <code>dw</code> is the derivative of <code>w</code> (Change to <code>w</code>) The derivative is also the slope of <code>w</code></p></li>
<li><p>Looks like greedy algorithms. the derivative give us the direction to improve our parameters.</p></li>
<li><p>The actual equations we will implement:</p>
<ul>
<li><code>w = w - alpha * d(J(w,b) / dw)</code> (how much the function slopes in the w direction)</li>
<li><code>b = b - alpha * d(J(w,b) / db)</code> (how much the function slopes in the d direction)</li>
</ul></li>
</ul>
<h3 id="derivatives">Derivatives</h3>
<ul>
<li>We will talk about some of required calculus.</li>
<li>You don't need to be a calculus geek to master deep learning but you'll need some skills from it.</li>
<li>Derivative of a linear line is its slope.
<ul>
<li>ex. <code>f(a) = 3a</code> <code>d(f(a))/d(a) = 3</code></li>
<li>if <code>a = 2</code> then <code>f(a) = 6</code></li>
<li>if we move a a little bit <code>a = 2.001</code> then <code>f(a) = 6.003</code> means that we multiplied the derivative (Slope) to the moved area and added it to the last result.</li>
</ul></li>
</ul>
<h3 id="more-derivatives-examples">More Derivatives examples</h3>
<ul>
<li><code>f(a) = a^2</code> ==&gt; <code>d(f(a))/d(a) = 2a</code>
<ul>
<li><code>a = 2</code> ==&gt; <code>f(a) = 4</code></li>
<li><code>a = 2.0001</code> ==&gt; <code>f(a) = 4.0004</code> approx.</li>
</ul></li>
<li><code>f(a) = a^3</code> ==&gt; <code>d(f(a))/d(a) = 3a^2</code></li>
<li><code>f(a) = log(a)</code> ==&gt; <code>d(f(a))/d(a) = 1/a</code></li>
<li>To conclude, Derivative is the slope and slope is different in different points in the function thats why the derivative is a function.</li>
</ul>
<h3 id="computation-graph">Computation graph</h3>
<ul>
<li>Its a graph that organizes the computation from left to right.
<ul>
<li><img src="images/02.png" /></li>
</ul></li>
</ul>
<h3 id="derivatives-with-a-computation-graph">Derivatives with a Computation Graph</h3>
<ul>
<li>Calculus chain rule says: If <code>x -&gt; y -&gt; z</code> (x effect y and y effects z) Then <code>d(z)/d(x) = d(z)/d(y) * d(y)/d(x)</code></li>
<li>The video illustrates a big example.
<ul>
<li><img src="images/03.png" /></li>
</ul></li>
<li>We compute the derivatives on a graph from right to left and it will be a lot more easier.</li>
<li><code>dvar</code> means the derivatives of a final output variable with respect to various intermediate quantities.</li>
</ul>
<h3 id="logistic-regression-gradient-descent">Logistic Regression Gradient Descent</h3>
<ul>
<li>In the video he discussed the derivatives of gradient decent example for one sample with two features <code>x1</code> and <code>x2</code>.
<ul>
<li><img src="images/04.png" /></li>
</ul></li>
</ul>
<h3 id="gradient-descent-on-m-examples">Gradient Descent on m Examples</h3>
<ul>
<li><p>Lets say we have these variables:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X1                  Feature</span><br><span class="line">X2                  Feature</span><br><span class="line">W1                  Weight of the first feature.</span><br><span class="line">W2                  Weight of the second feature.</span><br><span class="line">B                   Logistic Regression parameter.</span><br><span class="line">M                   Number of training examples</span><br><span class="line">Y(i)                Expected output of i</span><br></pre></td></tr></table></figure></li>
<li><p>So we have: <img src="images/09.png" /></p></li>
<li><p>Then from right to left we will calculate derivations compared to the result:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">d(a)  = d(l)/d(a) = -(y/a) + ((1-y)/(1-a))</span><br><span class="line">d(z)  = d(l)/d(z) = a - y</span><br><span class="line">d(W1) = X1 * d(z)</span><br><span class="line">d(W2) = X2 * d(z)</span><br><span class="line">d(B) = d(z)</span><br></pre></td></tr></table></figure></li>
<li><p>From the above we can conclude the logistic regression pseudo code:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">J = 0; dw1 = 0; dw2 =0; db = 0;                 # Devs.</span><br><span class="line">w1 = 0; w2 = 0; b=0;                            # Weights</span><br><span class="line">for i = 1 to m</span><br><span class="line">    # Forward pass</span><br><span class="line">    z(i) = W1*x1(i) + W2*x2(i) + b</span><br><span class="line">    a(i) = Sigmoid(z(i))</span><br><span class="line">    J += (Y(i)*log(a(i)) + (1-Y(i))*log(1-a(i)))</span><br><span class="line">    </span><br><span class="line">    # Backward pass</span><br><span class="line">    dz(i) = a(i) - Y(i)</span><br><span class="line">    dw1 += dz(i) * x1(i)</span><br><span class="line">    dw2 += dz(i) * x2(i)</span><br><span class="line">    db  += dz(i)</span><br><span class="line">J /= m</span><br><span class="line">dw1/= m</span><br><span class="line">dw2/= m</span><br><span class="line">db/= m</span><br><span class="line"></span><br><span class="line"># Gradient descent</span><br><span class="line">w1 = w1 - alpa * dw1</span><br><span class="line">w2 = w2 - alpa * dw2</span><br><span class="line">b = b - alpa * db</span><br></pre></td></tr></table></figure></li>
<li><p>The above code should run for some iterations to minimize error.</p></li>
<li><p>So there will be two inner loops to implement the logistic regression.</p></li>
<li><p>Vectorization is so important on deep learning to reduce loops. In the last code we can make the whole loop in one step using vectorization!</p></li>
</ul>
<h3 id="vectorization">Vectorization</h3>
<ul>
<li>Deep learning shines when the dataset are big. However for loops will make you wait a lot for a result. Thats why we need vectorization to get rid of some of our for loops.</li>
<li>NumPy library (dot) function is using vectorization by default.</li>
<li>The vectorization can be done on CPU or GPU thought the SIMD operation. But its faster on GPU.</li>
<li>Whenever possible avoid for loops.</li>
<li>Most of the NumPy library methods are vectorized version.</li>
</ul>
<h3 id="vectorizing-logistic-regression">Vectorizing Logistic Regression</h3>
<ul>
<li><p>We will implement Logistic Regression using one for loop then without any for loop.</p></li>
<li><p>As an input we have a matrix <code>X</code> and its <code>[Nx, m]</code> and a matrix <code>Y</code> and its <code>[Ny, m]</code>.</p></li>
<li><p>We will then compute at instance <code>[z1,z2...zm] = W' * X + [b,b,...b]</code>. This can be written in python as:</p>
<pre><code>      Z = np.dot(W.T,X) + b    # Vectorization, then broadcasting, Z shape is (1, m)
      A = 1 / 1 + np.exp(-Z)   # Vectorization, A shape is (1, m)</code></pre></li>
<li><p>Vectorizing Logistic Regression's Gradient Output:</p>
<pre><code>      dz = A - Y                  # Vectorization, dz shape is (1, m)
      dw = np.dot(X, dz.T) / m    # Vectorization, dw shape is (Nx, 1)
      db = dz.sum() / m           # Vectorization, dz shape is (1, 1)</code></pre></li>
</ul>
<h3 id="notes-on-python-and-numpy">Notes on Python and NumPy</h3>
<ul>
<li><p>In NumPy, <code>obj.sum(axis = 0)</code> sums the columns while <code>obj.sum(axis = 1)</code> sums the rows.</p></li>
<li><p>In NumPy, <code>obj.reshape(1,4)</code> changes the shape of the matrix by broadcasting the values.</p></li>
<li><p>Reshape is cheap in calculations so put it everywhere you're not sure about the calculations.</p></li>
<li><p>Broadcasting works when you do a matrix operation with matrices that doesn't match for the operation, in this case NumPy automatically makes the shapes ready for the operation by broadcasting the values.</p></li>
<li><p>Some tricks to eliminate all the strange bugs in the code:</p>
<ul>
<li>If you didn't specify the shape of a vector, it will take a shape of <code>(m,)</code> and the transpose operation won't work. You have to reshape it to <code>(m, 1)</code></li>
<li>Try to not use the rank one matrix in ANN</li>
<li>Don't hesitate to use <code>assert(a.shape == (5,1))</code> to check if your matrix shape is the required one.</li>
<li>If you've found a rank one matrix try to run reshape on it.</li>
</ul></li>
<li><p>Jupyter / IPython notebooks are so useful library in python that makes it easy to integrate code and document at the same time. It runs in the browser and doesn't need an IDE to run.</p>
<ul>
<li>To open Jupyter Notebook, open the command line and call: <code>jupyter-notebook</code> It should be installed to work.</li>
</ul></li>
<li><p>To Compute the derivative of Sigmoid:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s = sigmoid(x)</span><br><span class="line">ds = s * (1 - s)       # derivative  using calculus</span><br></pre></td></tr></table></figure></li>
<li><p>To make an image of <code>(width,height,depth)</code> be a vector, use this:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">v = image.reshape(image.shape[0]*image.shape[1]*image.shape[2],1)  #reshapes the image.</span><br></pre></td></tr></table></figure></li>
<li><p>Gradient descent converges faster after normalization of the input matrices.</p></li>
</ul>
<h3 id="general-notes">General Notes</h3>
<ul>
<li>The main steps for building a Neural Network are:
<ul>
<li>Define the model structure (such as number of input features and outputs)</li>
<li>Initialize the model's parameters.</li>
<li>Loop.
<ul>
<li>Calculate current loss (forward propagation)</li>
<li>Calculate current gradient (backward propagation)</li>
<li>Update parameters (gradient descent)</li>
</ul></li>
</ul></li>
<li>Preprocessing the dataset is important.</li>
<li>Tuning the learning rate (which is an example of a "hyperparameter") can make a big difference to the algorithm.</li>
<li><a href="kaggle.com">kaggle.com</a> is a good place for datasets and competitions.</li>
<li><a target="_blank" rel="noopener" href="https://www2.eecs.berkeley.edu/Faculty/Homepages/abbeel.html">Pieter Abbeel</a> is one of the best in deep reinforcement learning.</li>
</ul>
<h2 id="shallow-neural-networks">Shallow neural networks</h2>
<blockquote>
<p>Learn to build a neural network with one hidden layer, using forward propagation and backpropagation.</p>
</blockquote>
<h3 id="neural-networks-overview">Neural Networks Overview</h3>
<ul>
<li><p>In logistic regression we had:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X1  \  </span><br><span class="line">X2   ==&gt;  z = XW + B ==&gt; a = Sigmoid(z) ==&gt; l(a,Y)</span><br><span class="line">X3  /</span><br></pre></td></tr></table></figure></li>
<li><p>In neural networks with one layer we will have:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X1  \  </span><br><span class="line">X2   =&gt;  z1 = XW1 + B1 =&gt; a1 = Sigmoid(a1) =&gt; z2 = a1W2 + B2 =&gt; a2 = Sigmoid(z2) =&gt; l(a2,Y)</span><br><span class="line">X3  /</span><br></pre></td></tr></table></figure></li>
<li><p><code>X</code> is the input vector <code>(X1, X2, X3)</code>, and <code>Y</code> is the output variable <code>(1x1)</code></p></li>
<li><p>NN is stack of logistic regression objects.</p></li>
</ul>
<h3 id="neural-network-representation">Neural Network Representation</h3>
<ul>
<li>We will define the neural networks that has one hidden layer.</li>
<li>NN contains of input layers, hidden layers, output layers.</li>
<li>Hidden layer means we cant see that layers in the training set.</li>
<li><code>a0 = x</code> (the input layer)</li>
<li><code>a1</code> will represent the activation of the hidden neurons.</li>
<li><code>a2</code> will represent the output layer.</li>
<li>We are talking about 2 layers NN. The input layer isn't counted.</li>
</ul>
<h3 id="computing-a-neural-networks-output">Computing a Neural Network's Output</h3>
<ul>
<li>Equations of Hidden layers:
<ul>
<li><img src="images/05.png" /></li>
</ul></li>
<li>Here are some informations about the last image:
<ul>
<li><code>noOfHiddenNeurons = 4</code></li>
<li><code>Nx = 3</code></li>
<li>Shapes of the variables:
<ul>
<li><code>W1</code> is the matrix of the first hidden layer, it has a shape of <code>(noOfHiddenNeurons,nx)</code></li>
<li><code>b1</code> is the matrix of the first hidden layer, it has a shape of <code>(noOfHiddenNeurons,1)</code></li>
<li><code>z1</code> is the result of the equation <code>z1 = W1*X + b</code>, it has a shape of <code>(noOfHiddenNeurons,1)</code></li>
<li><code>a1</code> is the result of the equation <code>a1 = sigmoid(z1)</code>, it has a shape of <code>(noOfHiddenNeurons,1)</code></li>
<li><code>W2</code> is the matrix of the second hidden layer, it has a shape of <code>(1,noOfHiddenLayers)</code></li>
<li><code>b2</code> is the matrix of the second hidden layer, it has a shape of <code>(1,1)</code></li>
<li><code>z2</code> is the result of the equation <code>z2 = W2*a1 + b</code>, it has a shape of <code>(1,1)</code></li>
<li><code>a2</code> is the result of the equation <code>a2 = sigmoid(z2)</code>, it has a shape of <code>(1,1)</code></li>
</ul></li>
</ul></li>
</ul>
<h3 id="vectorizing-across-multiple-examples">Vectorizing across multiple examples</h3>
<ul>
<li><p>Pseudo code for forward propagation for the 2 layers NN:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">for i = 1 to m</span><br><span class="line">  z[1, i] = W1*x[i] + b1      # shape of z[1, i] is (noOfHiddenNeurons,1)</span><br><span class="line">  a[1, i] = sigmoid(z[1, i])  # shape of a[1, i] is (noOfHiddenNeurons,1)</span><br><span class="line">  z[2, i] = W2*a[1, i] + b2   # shape of z[2, i] is (1,1)</span><br><span class="line">  a[2, i] = sigmoid(z[2, i])  # shape of a[2, i] is (1,1)</span><br></pre></td></tr></table></figure></li>
<li><p>Lets say we have <code>X</code> on shape <code>(Nx,m)</code>. So the new pseudo code:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Z1 = W1X + b1     # shape of Z1 (noOfHiddenNeurons,m)</span><br><span class="line">A1 = sigmoid(Z1)  # shape of A1 (noOfHiddenNeurons,m)</span><br><span class="line">Z2 = W2A1 + b2    # shape of Z2 is (1,m)</span><br><span class="line">A2 = sigmoid(Z2)  # shape of A2 is (1,m)</span><br></pre></td></tr></table></figure></li>
<li><p>If you notice always m is the number of columns.</p></li>
<li><p>In the last example we can call <code>X</code> = <code>A0</code>. So the previous step can be rewritten as:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Z1 = W1A0 + b1    # shape of Z1 (noOfHiddenNeurons,m)</span><br><span class="line">A1 = sigmoid(Z1)  # shape of A1 (noOfHiddenNeurons,m)</span><br><span class="line">Z2 = W2A1 + b2    # shape of Z2 is (1,m)</span><br><span class="line">A2 = sigmoid(Z2)  # shape of A2 is (1,m)</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="activation-functions">Activation functions</h3>
<ul>
<li>So far we are using sigmoid, but in some cases other functions can be a lot better.</li>
<li>Sigmoid can lead us to gradient decent problem where the updates are so low.</li>
<li>Sigmoid activation function range is [0,1] <code>A = 1 / (1 + np.exp(-z)) # Where z is the input matrix</code></li>
<li>Tanh activation function range is [-1,1] (Shifted version of sigmoid function)
<ul>
<li><p>In NumPy we can implement Tanh using one of these methods: <code>A = (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z)) # Where z is the input matrix</code></p>
<p>Or <code>A = np.tanh(z)   # Where z is the input matrix</code></p></li>
</ul></li>
<li>It turns out that the tanh activation usually works better than sigmoid activation function for hidden units because the mean of its output is closer to zero, and so it centers the data better for the next layer.</li>
<li>Sigmoid or Tanh function disadvantage is that if the input is too small or too high, the slope will be near zero which will cause us the gradient decent problem.</li>
<li>One of the popular activation functions that solved the slow gradient decent is the RELU function. <code>RELU = max(0,z) # so if z is negative the slope is 0 and if z is positive the slope remains linear.</code></li>
<li>So here is some basic rule for choosing activation functions, if your classification is between 0 and 1, use the output activation as sigmoid and the others as RELU.</li>
<li>Leaky RELU activation function different of RELU is that if the input is negative the slope will be so small. It works as RELU but most people uses RELU. <code>Leaky_RELU = max(0.01z,z)  #the 0.01 can be a parameter for your algorithm.</code></li>
<li>In NN you will decide a lot of choices like:
<ul>
<li>No of hidden layers.</li>
<li>No of neurons in each hidden layer.</li>
<li>Learning rate. (The most important parameter)</li>
<li>Activation functions.</li>
<li>And others..</li>
</ul></li>
<li>It turns out there are no guide lines for that. You should try all activation functions for example.</li>
</ul>
<h3 id="why-do-you-need-non-linear-activation-functions">Why do you need non-linear activation functions?</h3>
<ul>
<li>If we removed the activation function from our algorithm that can be called linear activation function.</li>
<li>Linear activation function will output linear activations
<ul>
<li>Whatever hidden layers you add, the activation will be always linear like logistic regression (So its useless in a lot of complex problems)</li>
</ul></li>
<li>You might use linear activation function in one place - in the output layer if the output is real numbers (regression problem). But even in this case if the output value is non-negative you could use RELU instead.</li>
</ul>
<h3 id="derivatives-of-activation-functions">Derivatives of activation functions</h3>
<ul>
<li><p>Derivation of Sigmoid activation function:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">g(z) = 1 / (1 + np.exp(-z))</span><br><span class="line">g&#x27;(z) = (1 / (1 + np.exp(-z))) * (1 - (1 / (1 + np.exp(-z))))</span><br><span class="line">g&#x27;(z) = g(z) * (1 - g(z))</span><br></pre></td></tr></table></figure></li>
<li><p>Derivation of Tanh activation function:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">g(z)  = (e^z - e^-z) / (e^z + e^-z)</span><br><span class="line">g&#x27;(z) = 1 - np.tanh(z)^2 = 1 - g(z)^2</span><br></pre></td></tr></table></figure></li>
<li><p>Derivation of RELU activation function:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">g(z)  = np.maximum(0,z)</span><br><span class="line">g&#x27;(z) = &#123; 0  if z &lt; 0</span><br><span class="line">          1  if z &gt;= 0  &#125;</span><br></pre></td></tr></table></figure></li>
<li><p>Derivation of leaky RELU activation function:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">g(z)  = np.maximum(0.01 * z, z)</span><br><span class="line">g&#x27;(z) = &#123; 0.01  if z &lt; 0</span><br><span class="line">          1     if z &gt;= 0   &#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="gradient-descent-for-neural-networks">Gradient descent for Neural Networks</h3>
<ul>
<li><p>In this section we will have the full back propagation of the neural network (Just the equations with no explanations).</p></li>
<li><p>Gradient descent algorithm:</p>
<ul>
<li><p>NN parameters:</p>
<ul>
<li><code>n[0] = Nx</code></li>
<li><code>n[1] = NoOfHiddenNeurons</code></li>
<li><code>n[2] = NoOfOutputNeurons = 1</code></li>
<li><code>W1</code> shape is <code>(n[1],n[0])</code></li>
<li><code>b1</code> shape is <code>(n[1],1)</code></li>
<li><code>W2</code> shape is <code>(n[2],n[1])</code></li>
<li><code>b2</code> shape is <code>(n[2],1)</code></li>
</ul></li>
<li><p>Cost function <code>I =  I(W1, b1, W2, b2) = (1/m) * Sum(L(Y,A2))</code></p></li>
<li><p>Then Gradient descent:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Repeat:</span><br><span class="line">        Compute predictions (y&#x27;[i], i = 0,...m)</span><br><span class="line">        Get derivatives: dW1, db1, dW2, db2</span><br><span class="line">        Update: W1 = W1 - LearningRate * dW1</span><br><span class="line">                b1 = b1 - LearningRate * db1</span><br><span class="line">                W2 = W2 - LearningRate * dW2</span><br><span class="line">                b2 = b2 - LearningRate * db2</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li><p>Forward propagation:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Z1 = W1A0 + b1    # A0 is X</span><br><span class="line">A1 = g1(Z1)</span><br><span class="line">Z2 = W2A1 + b2</span><br><span class="line">A2 = Sigmoid(Z2)      # Sigmoid because the output is between 0 and 1</span><br></pre></td></tr></table></figure></li>
<li><p>Back propagation (The new thing / derivations):</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dZ2 = A2 - Y      # derivative of cost function we used * derivative of the sigmoid function</span><br><span class="line">dW2 = (dZ2 * A1.T) / m</span><br><span class="line">db2 = Sum(dZ2) / m</span><br><span class="line">dZ1 = (W2.T * dZ2) * g&#x27;1(Z1)  # element wise product (*)</span><br><span class="line">dW2 = (dZ1 * A0.T) / m   # A0 = X</span><br><span class="line">db2 = Sum(dZ1) / m</span><br><span class="line"># Hint there are transposes when you are trying to multiplicate because these are matrices.</span><br></pre></td></tr></table></figure></li>
<li><p>How we derived the 6 equations of the back propagation:</p>
<ul>
<li><img src="images/06.png" /></li>
</ul></li>
</ul>
<h3 id="random-initialization">Random Initialization</h3>
<ul>
<li><p>In logistic regression it wasn't important to initialize the weights randomly, while in NN we have to initialize them randomly.</p></li>
<li><p>If we initialize all the weights with zeros in NN it won't work (initializing bias with zero is OK):</p>
<ul>
<li>all hidden units will be completely identical (symmetric) - compute exactly the same function</li>
<li>on each gradient descent iteration all the hidden units will always update the same</li>
</ul></li>
<li><p>To solve this we initialize the W's with a small random numbers:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W1 = np.random.randn((2,2)) * 0.01    # 0.01 to make it small enough</span><br><span class="line">b1 = np.zeros((2,1))                  # its ok to have b as zero, it won&#x27;t get us to the symmetry breaking problem</span><br></pre></td></tr></table></figure></li>
<li><p>We need small values because in sigmoid (or tanh), for example, if the weight is too large you are more likely to end up even at the very start of training with very large values of Z. Which causes your tanh or your sigmoid activation function to be saturated, thus slowing down learning. If you don't have any sigmoid or tanh activation functions throughout your neural network, this is less of an issue.</p></li>
<li><p>Constant 0.01 is alright for 1 hidden layer networks, but if the NN is deep this number can be changed but it will always be a small number.</p></li>
</ul>
<h2 id="deep-neural-networks">Deep Neural Networks</h2>
<blockquote>
<p>Understand the key computations underlying deep learning, use them to build and train deep neural networks, and apply it to computer vision.</p>
</blockquote>
<h3 id="deep-l-layer-neural-network">Deep L-layer neural network</h3>
<ul>
<li>Shallow NN is a NN with one or two layers.</li>
<li>Deep NN is a NN with three or more layers.</li>
<li>We will use the notation <code>L</code> to denote the number of layers in a NN.</li>
<li><code>n[l]</code> is the number of neurons in a specific layer <code>l</code>.</li>
<li><code>n[0]</code> denotes the number of neurons input layer. <code>n[L]</code> denotes the number of neurons in output layer.</li>
<li><code>g[l]</code> is the activation function.</li>
<li><code>a[l] = g[l](z[l])</code></li>
<li><code>w[l]</code> weights is used for <code>z[l]</code></li>
<li><code>x = a[0]</code>, <code>a[l] = y'</code></li>
<li>These were the notation we will use for deep neural network.</li>
<li>So we have:
<ul>
<li>A vector <code>n</code> of shape <code>(1, NoOfLayers+1)</code></li>
<li>A vector <code>g</code> of shape <code>(1, NoOfLayers)</code></li>
<li>A list of different shapes <code>w</code> based on the number of neurons on the previous and the current layer.</li>
<li>A list of different shapes <code>b</code> based on the number of neurons on the current layer.</li>
</ul></li>
</ul>
<h3 id="forward-propagation-in-a-deep-network">Forward Propagation in a Deep Network</h3>
<ul>
<li><p>Forward propagation General rule for one input:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">z[l] = W[l]a[l-1] + b[l]</span><br><span class="line">a[l] = g[l](a[l])</span><br></pre></td></tr></table></figure></li>
<li><p>Forward propagation General rule for <code>m</code> inputs:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Z[l] = W[l]A[l-1] + B[l]</span><br><span class="line">A[l] = g[l](A[l])</span><br></pre></td></tr></table></figure></li>
<li><p>We can't compute the whole layers forward propagation without a for loop so its OK to have a for loop here.</p></li>
<li><p>The dimensions of the matrices are so important you need to figure it out.</p></li>
</ul>
<h3 id="getting-your-matrix-dimensions-right">Getting your matrix dimensions right</h3>
<ul>
<li>The best way to debug your matrices dimensions is by a pencil and paper.</li>
<li>Dimension of <code>W</code> is <code>(n[l],n[l-1])</code> . Can be thought by Right to left.</li>
<li>Dimension of <code>b</code> is <code>(n[l],1)</code></li>
<li><code>dw</code> has the same shape as <code>W</code>, while <code>db</code> is the same shape as <code>b</code></li>
<li>Dimension of <code>Z[l],</code> <code>A[l]</code>, <code>dZ[l]</code>, and <code>dA[l]</code> is <code>(n[l],m)</code></li>
</ul>
<h3 id="why-deep-representations">Why deep representations?</h3>
<ul>
<li>Why deep NN works well, we will discuss this question in this section.</li>
<li>Deep NN makes relations with data from simpler to complex. In each layer it tries to make a relations between the previous layer.</li>
<li>Face recognition application:
<ul>
<li>Image ==&gt; Edges ==&gt; Face parts ==&gt; Faces ==&gt; desired face</li>
</ul></li>
<li>Audio recognition application:
<ul>
<li>Audio ==&gt; Low level sound features like (sss,bb) ==&gt; Phonemes ==&gt; Words ==&gt; Sentences</li>
</ul></li>
<li>Neural Researchers thinks that deep neural networks thinks like brains (Simple ==&gt; Complex)</li>
<li>Circuit theory and deep learning:
<ul>
<li><img src="images/07.png" /></li>
</ul></li>
<li>When starting on an application don't start directly by dozens of hidden layers. Try the simplest solutions (L Regression) then try the parameters then try the shallow neural network and so on.</li>
</ul>
<h3 id="building-blocks-of-deep-neural-networks">Building blocks of deep neural networks</h3>
<ul>
<li>Forward and back propagation for a layer l:
<ul>
<li><img src="images/10.png" title="fig:" alt="Untitled" /></li>
</ul></li>
<li>Deep NN blocks:
<ul>
<li><img src="images/08.png" /></li>
</ul></li>
</ul>
<h3 id="forward-and-backward-propagation">Forward and Backward Propagation</h3>
<ul>
<li><p>Pseudo code for forward propagation for layer l:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Input  A[l-1]</span><br><span class="line">Z[l] = W[l]A[l-1] + b[l]</span><br><span class="line">A[l] = g[l](Z[l])</span><br><span class="line">Output A[l], cache(Z[l])</span><br></pre></td></tr></table></figure></li>
<li><p>Pseudo code for back propagation for layer l:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Input da[l], Caches</span><br><span class="line">dZ[l] = dA[l] * g&#x27;[l](Z[l])</span><br><span class="line">dW[l] = (dZ[l]A[l-1].T) / m</span><br><span class="line">db[l] = sum(dZ[l])/m                # Dont forget axis=1, keepdims=True</span><br><span class="line">dA[l-1] = w[l].T * dZ[1]            # The multiplication here are a dot product.</span><br><span class="line">Output dA[l-1], dW[l], db[l]</span><br></pre></td></tr></table></figure></li>
<li><p>If we have used our loss function then:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dA[L] = (-(y/a) + ((1-y)/(1-a)))</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="parameters-vs-hyperparameters">Parameters vs Hyperparameters</h3>
<ul>
<li>Main parameters of the NN is <code>W</code> and <code>b</code></li>
<li>Hyper parameters (parameters that control the algorithm) are like:
<ul>
<li>Learning rate.</li>
<li>Number of iteration.</li>
<li>Number of hidden layers <code>L</code>.</li>
<li>Number of hidden units <code>n</code>.</li>
<li>Choice of activation functions.</li>
</ul></li>
<li>You have to try values yourself of hyper parameters.</li>
<li>In the old days they thought that learning rate is a parameter while now all knows its a hyper parameter.</li>
<li>On the next course we will see how to optimize hyperparameters.</li>
</ul>
<h3 id="what-does-this-have-to-do-with-the-brain">What does this have to do with the brain</h3>
<ul>
<li>No Human today understand how a human brain neuron works.</li>
<li>No Human today know exactly how many neurons on the brain.</li>
<li>NN is a small representation of how brain work. The most near model of human brain is in the computer vision (CNN)</li>
</ul>
<h2 id="extra-ian-goodfellow-interview">Extra: Ian Goodfellow interview</h2>
<ul>
<li>Ian is one of the world's most visible deep learning researchers.</li>
<li>Ian is mainly working with generative models. He is the creator of GANs.</li>
<li>We need to stabilize GANs. Stabilized GANs can become the best generative models.</li>
<li>Ian wrote the first textbook on the modern version of deep learning with Yoshua Bengio and Aaron Courville.</li>
<li>Ian worked with <a href="OpenAI.com">OpenAI.com</a> and Google on ML and NN applications.</li>
<li>Ian tells all who wants to get into AI to get a Ph.D. or post your code on Github and the companies will find you.</li>
<li>Ian thinks that we need to start anticipating security problems with ML now and make sure that these algorithms are secure from the start instead of trying to patch it in retroactively years later.</li>
</ul>
<p><br><br> <br><br> These Notes were made by <a href="mailto:mma18@fayoum.edu.eg">Mahmoud Badry</a> <span class="citation" data-cites="2017">@2017</span></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Course-Note/" rel="tag"># Course Note</a>
              <a href="/tags/Project/" rel="tag"># Project</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/Trigger-word-detection/2019/03/29/" rel="prev" title="Trigger word detection">
      <i class="fa fa-chevron-left"></i> Trigger word detection
    </a></div>
      <div class="post-nav-item">
    <a href="/Improving-Deep-Neural-Networks/2019/03/29/" rel="next" title="Improving Deep Neural Networks">
      Improving Deep Neural Networks <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#table-of-contents"><span class="nav-number">1.</span> <span class="nav-text">Table of contents</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#course-summary"><span class="nav-number">2.</span> <span class="nav-text">Course summary</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#introduction-to-deep-learning"><span class="nav-number">3.</span> <span class="nav-text">Introduction to deep learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#what-is-a-neural-network-nn"><span class="nav-number">3.1.</span> <span class="nav-text">What is a (Neural Network) NN?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#supervised-learning-with-neural-networks"><span class="nav-number">3.2.</span> <span class="nav-text">Supervised learning with neural networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#why-is-deep-learning-taking-off"><span class="nav-number">3.3.</span> <span class="nav-text">Why is deep learning taking off?</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#neural-networks-basics"><span class="nav-number">4.</span> <span class="nav-text">Neural Networks Basics</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#binary-classification"><span class="nav-number">4.1.</span> <span class="nav-text">Binary classification</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#logistic-regression"><span class="nav-number">4.2.</span> <span class="nav-text">Logistic regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#logistic-regression-cost-function"><span class="nav-number">4.3.</span> <span class="nav-text">Logistic regression cost function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gradient-descent"><span class="nav-number">4.4.</span> <span class="nav-text">Gradient Descent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#derivatives"><span class="nav-number">4.5.</span> <span class="nav-text">Derivatives</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#more-derivatives-examples"><span class="nav-number">4.6.</span> <span class="nav-text">More Derivatives examples</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#computation-graph"><span class="nav-number">4.7.</span> <span class="nav-text">Computation graph</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#derivatives-with-a-computation-graph"><span class="nav-number">4.8.</span> <span class="nav-text">Derivatives with a Computation Graph</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#logistic-regression-gradient-descent"><span class="nav-number">4.9.</span> <span class="nav-text">Logistic Regression Gradient Descent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gradient-descent-on-m-examples"><span class="nav-number">4.10.</span> <span class="nav-text">Gradient Descent on m Examples</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#vectorization"><span class="nav-number">4.11.</span> <span class="nav-text">Vectorization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#vectorizing-logistic-regression"><span class="nav-number">4.12.</span> <span class="nav-text">Vectorizing Logistic Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#notes-on-python-and-numpy"><span class="nav-number">4.13.</span> <span class="nav-text">Notes on Python and NumPy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#general-notes"><span class="nav-number">4.14.</span> <span class="nav-text">General Notes</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#shallow-neural-networks"><span class="nav-number">5.</span> <span class="nav-text">Shallow neural networks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#neural-networks-overview"><span class="nav-number">5.1.</span> <span class="nav-text">Neural Networks Overview</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#neural-network-representation"><span class="nav-number">5.2.</span> <span class="nav-text">Neural Network Representation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#computing-a-neural-networks-output"><span class="nav-number">5.3.</span> <span class="nav-text">Computing a Neural Network&#39;s Output</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#vectorizing-across-multiple-examples"><span class="nav-number">5.4.</span> <span class="nav-text">Vectorizing across multiple examples</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#activation-functions"><span class="nav-number">5.5.</span> <span class="nav-text">Activation functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#why-do-you-need-non-linear-activation-functions"><span class="nav-number">5.6.</span> <span class="nav-text">Why do you need non-linear activation functions?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#derivatives-of-activation-functions"><span class="nav-number">5.7.</span> <span class="nav-text">Derivatives of activation functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gradient-descent-for-neural-networks"><span class="nav-number">5.8.</span> <span class="nav-text">Gradient descent for Neural Networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#random-initialization"><span class="nav-number">5.9.</span> <span class="nav-text">Random Initialization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#deep-neural-networks"><span class="nav-number">6.</span> <span class="nav-text">Deep Neural Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#deep-l-layer-neural-network"><span class="nav-number">6.1.</span> <span class="nav-text">Deep L-layer neural network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#forward-propagation-in-a-deep-network"><span class="nav-number">6.2.</span> <span class="nav-text">Forward Propagation in a Deep Network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#getting-your-matrix-dimensions-right"><span class="nav-number">6.3.</span> <span class="nav-text">Getting your matrix dimensions right</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#why-deep-representations"><span class="nav-number">6.4.</span> <span class="nav-text">Why deep representations?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#building-blocks-of-deep-neural-networks"><span class="nav-number">6.5.</span> <span class="nav-text">Building blocks of deep neural networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#forward-and-backward-propagation"><span class="nav-number">6.6.</span> <span class="nav-text">Forward and Backward Propagation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#parameters-vs-hyperparameters"><span class="nav-number">6.7.</span> <span class="nav-text">Parameters vs Hyperparameters</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#what-does-this-have-to-do-with-the-brain"><span class="nav-number">6.8.</span> <span class="nav-text">What does this have to do with the brain</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#extra-ian-goodfellow-interview"><span class="nav-number">7.</span> <span class="nav-text">Extra: Ian Goodfellow interview</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">227</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">46</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zhangruochi" title="GitHub â†’ https:&#x2F;&#x2F;github.com&#x2F;zhangruochi" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zrc720@gmail.com" title="E-Mail â†’ mailto:zrc720@gmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.healthinformaticslab.org/" title="http:&#x2F;&#x2F;www.healthinformaticslab.org" rel="noopener" target="_blank">HILab</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.shihaizhou.com/" title="http:&#x2F;&#x2F;www.shihaizhou.com" rel="noopener" target="_blank">Rose</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/cherish_CX/" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;cherish_CX&#x2F;" rel="noopener" target="_blank">Chunxia</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 â€“ 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
      appKey     : 'GL6JvT9DgGxqYrY5Vj6bXVuv',
      placeholder: "Thank you for your reply",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'en' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
